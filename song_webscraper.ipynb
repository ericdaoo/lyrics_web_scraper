{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3aaa4efe-821f-4996-8803-8db8c12efcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import requests\n",
    "import shutil\n",
    "import glob\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import pywordseg as seg  # https://pypi.org/project/pywordseg/\n",
    "import nltk\n",
    "import contractions\n",
    "from nltk.corpus import stopwords \n",
    "#nltk.download('vader_lexicon')\n",
    "#nltk.download('popular')\n",
    "#%pip install xlsxwriter\n",
    "# from TCSP import read_stopwords_list\n",
    "# %pip install tcsp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63592a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://realpython.com/python-nltk-sentiment-analysis/\n",
    "# https://machinelearningmastery.com/clean-text-machine-learning-python/\n",
    "# https://www.guru99.com/pos-tagging-chunking-nltk.html\n",
    "# https://www.nltk.org/api/nltk.tokenize.html\n",
    "# https://github.com/bryanchw/Traditional-Chinese-Stopwords-and-Punctuations-Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ed52ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# script_dir = os.path.dirname(r\"C:\\Users\\ericdao\\Downloads\\\\\") #to specify another directory to output to\n",
    "\n",
    "script_dir = os.getcwd()\n",
    "out_folder = str(datetime.date.today()) + \"_output\"\n",
    "abs_path = os.path.join(script_dir, out_folder)\n",
    "# or can specifiy a specific folder if working on a previous existing output \n",
    "# abs_path = \"2022-04-29_output\" \n",
    "\n",
    "if os.path.exists(abs_path):\n",
    "    shutil.rmtree(abs_path)\n",
    "os.mkdir(abs_path)\n",
    "\n",
    "\n",
    "# set up webscraping driver\n",
    "chromedriver = './chromedriver_100' #mac doesn't requite extenstion in name, whereas, windows does. Driver download page: https://chromedriver.chromium.org/downloads\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_experimental_option('excludeSwitches', ['enable-logging'])  #prevents logging of some unecessary driver related logs\n",
    "driver = webdriver.Chrome(chromedriver, options=options) \n",
    "\n",
    "# url = \"https://google.com\"\n",
    "# driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e83dd94-a2be-4920-af66-788c244385e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#grab the US song titles\n",
    "\n",
    "page = requests.get(\"https://www.billboard.com/charts/year-end/2021/hot-100-songs\")\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "job_elements = soup.find_all(\"h3\", class_=\"c-title\") \n",
    "song_titles = []\n",
    "for song_title in job_elements:\n",
    "    song_titles.append(song_title.text.strip())\n",
    "\n",
    "song_titles_100 = song_titles[:100] #verify that that this list contains all the song titles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "544613de-99bf-4edd-8a2f-61c0d4bc3519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error Drivers License Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"div[jsname=\"WbKHeb\"]\"}\n",
      "  (Session info: chrome=100.0.4896.127)\n",
      "\n",
      "error Beautiful Mistakes Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"div[jsname=\"WbKHeb\"]\"}\n",
      "  (Session info: chrome=100.0.4896.127)\n",
      "\n",
      "error Kings & Queens Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"div[jsname=\"WbKHeb\"]\"}\n",
      "  (Session info: chrome=100.0.4896.127)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use song titles to search the lyrics. Will need to manually fetch and input any song lyrics that weren't able to be automatically web scraped\n",
    "\n",
    "file_name = \"us_song_lyrics_2021\"\n",
    "lyrics = []\n",
    "with open((abs_path + \"//\" + file_name +'.csv'), 'w', newline='') as f:\n",
    "    write = csv.writer(f)\n",
    "    for song in song_titles_100:\n",
    "        try:\n",
    "            matched_elements = driver.get(\"https://www.google.com/search?q=\" + song + \" lyrics\")\n",
    "            time.sleep(1)\n",
    "            x = driver.find_element_by_css_selector('div[jsname=\"WbKHeb\"]')\n",
    "            lyrics.append(x.text)\n",
    "            write.writerow([x.text])\n",
    "        except Exception as e:\n",
    "            print(\"error | \",song, \" | \", e) # there will be some lyrics that won't automatically show up in google's first search result. These will need to be manually found and inserted into the us_song_lyrics csv file\n",
    "            pass\n",
    "    f.close()    \n",
    "    print(len(lyrics) + \" songs' lyrics retrieved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "acc98dc1-a2af-47b3-a054-97c7d2a6100d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# webscaper for tw song lyrics. Some song indexes will give an error like \"frame disconnected\", I think this relates to maybe an error casued by trying to scrape something when the page hasn't loaded completely yet maybe. I played around with putting wait intervals between different web scraping steps, and that seemed to help, but I need to spend more time to figure out exactly what causes that error in this script section\n",
    "\n",
    "driver = webdriver.Chrome(chromedriver, options=options) \n",
    "\n",
    "file_name = \"tw_song_lyrics_2020\"\n",
    "songs = []\n",
    "with open((abs_path + \"//\" + file_name +'.csv'), 'w', newline='', encoding=\"UTF-8\") as f:\n",
    "    write = csv.writer(f)\n",
    "    for i in range(0,202,2):\n",
    "        try:\n",
    "            driver.get(\"https://kma.kkbox.com/charts/yearly/newrelease?cate=297&lang=tc&terr=tw&year=2020#\")\n",
    "            time.sleep(5)\n",
    "            page = driver.find_elements_by_class_name(\"charts-list-cover\")\n",
    "            time.sleep(2)\n",
    "            new_page = page[i].click()\n",
    "            time.sleep(3)\n",
    "            page = requests.get(driver.current_url)\n",
    "            time.sleep(3)\n",
    "\n",
    "            soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "            time.sleep(2)\n",
    "            job_elements = soup.find(\"div\", class_=\"lyrics\")\n",
    "            time.sleep(3)\n",
    "            raw_text = job_elements.getText().splitlines()\n",
    "            while True:\n",
    "                for a in range(len(raw_text)): \n",
    "                    author_line_1 = raw_text[a].find('：') #-1 will be ruturned for find() if no match is found\n",
    "                    author_line_2 = raw_text[a].find(':')\n",
    "                    if (author_line_1 == -1 and author_line_2 == -1) and raw_text[a] != '': #this will skip those first few lines with the \"：\" symbol and also the blank lines\n",
    "                        break #once the first line is found with the ： and isn't blank, the loop will break and now a is the index of the start of the actual song to extract\n",
    "                break\n",
    "            text = raw_text[a:]\n",
    "            text = str(\"\\n\".join(text))\n",
    "            text = text.replace(\"\\n\", \" \")\n",
    "            songs.append(text)\n",
    "            write.writerow([text])\n",
    "            print(i)\n",
    "        except Exception as e:\n",
    "            print(\"error\",i, e)\n",
    "            pass\n",
    "    f.close()  \n",
    "    print(len(songs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "de482f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# manually fetch any tw song indexes that weren't sucessfully pulled and append to existing csv\n",
    "\n",
    "driver = webdriver.Chrome(chromedriver, options=options) \n",
    "\n",
    "file_name = \"tw_song_lyrics_2021\"\n",
    "songs = []\n",
    "with open((abs_path + \"//\" + file_name +'.csv'), 'a', newline='', encoding=\"UTF-8\") as f: #adjust directory path\n",
    "    write = csv.writer(f)\n",
    "    for i in ([22]):\n",
    "        try:\n",
    "            driver.get(\"https://kma.kkbox.com/charts/yearly/newrelease?cate=297&lang=tc&terr=tw&year=2021#\") #adjust year\n",
    "            time.sleep(5)\n",
    "            page = driver.find_elements_by_class_name(\"charts-list-cover\")\n",
    "            time.sleep(2)\n",
    "            new_page = page[i].click()\n",
    "            time.sleep(3)\n",
    "            page = requests.get(driver.current_url)\n",
    "            time.sleep(3)\n",
    "\n",
    "            soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "            time.sleep(2)\n",
    "            job_elements = soup.find(\"div\", class_=\"lyrics\")\n",
    "            time.sleep(3)\n",
    "            raw_text = job_elements.getText().splitlines()\n",
    "            while True:\n",
    "                for a in range(len(raw_text)): \n",
    "                    author_line_1 = raw_text[a].find('：') #-1 will be ruturned for find() if no match is found\n",
    "                    author_line_2 = raw_text[a].find(':')\n",
    "                    if (author_line_1 == -1 and author_line_2 == -1) and raw_text[a] != '': #this will skip those first few lines with the \"：\" symbol and also the blank lines\n",
    "                        break #once the first line is found with the ： and isn't blank, the loop will break and now a is the index of the start of the actual song to extract\n",
    "                break\n",
    "            text = raw_text[a:]\n",
    "            text = str(\"\\n\".join(text))\n",
    "            text = text.replace(\"\\n\", \" \")\n",
    "            songs.append(text)\n",
    "            write.writerow([text])\n",
    "            print(i)\n",
    "        except Exception as e:\n",
    "            print(\"error\",i, e)\n",
    "            pass\n",
    "    f.close()  \n",
    "    print(len(songs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert manually obtained lyrics into csv\n",
    "\n",
    "lyrics = \"\"\"我們走過了多少的傷害\n",
    "然後經歷了又一次失敗\n",
    "I know i’ll be alright\n",
    "Everything will be alright alright\n",
    "\n",
    "我們走過了多少的傷害\n",
    "然後經歷了又一次失敗\n",
    "I know i’ll be alright\n",
    "Everything will be alright alright be alright\n",
    "\n",
    "Ya 放輕鬆 全部重新來過\n",
    "繼續 向前走 把煩惱丟走\n",
    "不再留戀 別再回頭\n",
    "We moving on run n run like a circle\n",
    "認真的去感受頻率\n",
    "希望停止那些回憶\n",
    "回憶都只是過去 讓他過去 過去\n",
    "\n",
    "我們走過了多少的傷害（傷害）\n",
    "然後經歷了又一次失敗\n",
    "I know i’ll be alright（i know i’ll be alright）\n",
    "Everything will be alright alright\n",
    "\n",
    "我們走過了多少的傷害（傷害）\n",
    "然後經歷了又一次失敗\n",
    "I know i’ll be alright（i know i’ll be alright）\n",
    "Everything will be alright alright be alright\n",
    "Be alright i i’ll be alright\n",
    "Be alright i i’ll be alright（i know i’ll be alright）\n",
    "Be alright i i’ll be alright\n",
    "Be alright i i’ll be alright\n",
    "（everything will be alright alright be alright）\n",
    "\n",
    "獨自一人走在大街\n",
    "孤獨一人聽著音樂\n",
    "不敢再犯錯我一步一步的走\n",
    "曾經想要解脫 但又從頭來過\n",
    "不再回頭 保持初心向前走\n",
    "所有煩惱拋腦後\n",
    "認真的過每天生活\n",
    "\n",
    "I’ll go insane if i stay\n",
    "All this way with no change all day\n",
    "Undo the pain\n",
    "Freshen up\n",
    "Get back up\n",
    "Moving on go on\n",
    "\n",
    "我們走過了多少的傷害（傷害）\n",
    "然後經歷了又一次失敗\n",
    "I know i’ll be alright（i know i’ll be alright）\n",
    "Everything will be alright alright（be alright）\n",
    "我們走過了多少的傷害（傷害）\n",
    "然後經歷了又一次失敗\n",
    "I know i’ll be alright（i know i’ll be alright）\n",
    "Everything will be alright alright be alright\n",
    "\n",
    "Be alright i i’ll be alright\n",
    "Be alright i i’ll be alright（i know i’ll be alright）\n",
    "Be alright i i’ll be alright\n",
    "Be alright i i’ll be alright\n",
    "（everything will be alright alright be alright）\n",
    "\n",
    "Be alright i i’ll be alright\n",
    "Be alright i i’ll be alright（i know i’ll be alright）\n",
    "Be alright i i’ll be alright\n",
    "Be alright i i’ll be alright\n",
    "Everything will be alright alright be alright\"\"\"\n",
    "\n",
    "raw_text = lyrics.splitlines()\n",
    "\n",
    "with open((abs_path + \"//\" + file_name +'.csv'), 'a', newline='', encoding=\"UTF-8\") as f:\n",
    "    write = csv.writer(f)\n",
    "    while True:\n",
    "        for a in range(len(raw_text)): \n",
    "            author_line_1 = raw_text[a].find('：') #-1 will be ruturned for find() if no match is found\n",
    "            author_line_2 = raw_text[a].find(':')\n",
    "            if (author_line_1 == -1 and author_line_2 == -1) and raw_text[a] != '': #this will skip those first few lines with the \"：\" symbol and also the blank lines\n",
    "                break #once the first line is found with the ： and isn't blank, the loop will break and now a is the index of the start of the actual song to extract\n",
    "        break\n",
    "    text = raw_text[a:]\n",
    "    text = str(\"\\n\".join(text))\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    songs.append(text)\n",
    "    write.writerow([text])          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "898ffa83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output each individual word count for tw words\n",
    "\n",
    "file_name = \"tw_song_lyrics_2020\"\n",
    "\n",
    "\n",
    "# flattten song lyric list to prepare for segmentation\n",
    "df = pd.read_csv(abs_path + \"//\" + file_name + \".csv\", encoding=\"UTF-8\", header=None)\n",
    "songs_to_segment = df.values.tolist()\n",
    "flattened_song_list = ' '.join([data for song in songs_to_segment for data in song])\n",
    "\n",
    "\n",
    "# declare the segmentor. embedding=\"w2v\" will use the baseline model, \"elmo\" is supposely more accurate, but slower\n",
    "segmentor = seg.Wordseg(batch_size=64, device=\"cpu\", embedding='w2v', elmo_use_cuda=False, mode=\"TW\")\n",
    "\n",
    "# input is a flattened string of all the sentences.\n",
    "lyric_segmentation_result = segmentor.cut([flattened_song_list])[0]\n",
    "# test = segmentor.cut([\"今天天氣真好啊! 潮水退了就知道，誰沒穿褲子。\"])\n",
    "\n",
    "\n",
    "# remove punctuation characters\n",
    "punctuation = pd.read_csv(\"tw_punctuation.csv\", encoding=\"UTF-8\", header=None)\n",
    "punctuation = punctuation[0].values.tolist()\n",
    "lyric_segmentation = [word.strip().lower() for word in lyric_segmentation_result if not word.strip().lower() in punctuation and not \"'\" in word.strip().lower() and not \",\" in word.strip().lower() and not \"’\" in word.strip().lower()]\n",
    "# lyric_segmentation = [word.strip().lower() for word in lyric_segmentation if not \"'\" in word and not \",\" in word]\n",
    "# lyric_segmentation = [word.lower() for word in lyric_segmentation not word.__contains__(\"'\")]\n",
    "\n",
    "\n",
    "# remove stop words\n",
    "stop_words = pd.read_csv(\"tw_stopwords.csv\", encoding=\"UTF-8\", header=None)\n",
    "stop_words = stop_words[0].values.tolist()\n",
    "lyric_rm_stopwords_segmentation = [word.strip().lower() for word in lyric_segmentation_result if not word.strip().lower() in stop_words and not \"'\" in word.strip().lower() and not \",\" in word.strip().lower() and not \"’\" in word.strip().lower()]\n",
    "# lyric_rm_stopwords_segmentation = [word.lower() for word in lyric_rm_stopwords_segmentation if word not in \"'\"]\n",
    "\n",
    "# calculate each word's word count\n",
    "def countOccurrence(a):\n",
    "  k = {}\n",
    "  for j in a:\n",
    "    if j in k:\n",
    "      k[j] +=1\n",
    "    else:\n",
    "      k[j] =1\n",
    "  return k\n",
    "\n",
    "lyric_segmentation_count = countOccurrence(lyric_segmentation)\n",
    "lyric_rm_stopwords_segmentation_count = countOccurrence(lyric_rm_stopwords_segmentation)\n",
    "\n",
    "# sort word count\n",
    "sorted_keys = sorted(lyric_segmentation_count, key=lyric_segmentation_count.get, reverse=True)\n",
    "sorted_lyric_segmentation_count = {}\n",
    "for w in sorted_keys:\n",
    "    sorted_lyric_segmentation_count[w] = lyric_segmentation_count[w]\n",
    "\n",
    "sorted_keys_rm_stopwords = sorted(lyric_rm_stopwords_segmentation_count, key=lyric_rm_stopwords_segmentation_count.get, reverse=True)\n",
    "sorted_rm_stopwords_lyric_segmentation_count = {}\n",
    "for w in sorted_keys_rm_stopwords:\n",
    "    sorted_rm_stopwords_lyric_segmentation_count[w] = lyric_rm_stopwords_segmentation_count[w]\n",
    "\n",
    "# convert dictionary to dataframe\n",
    "sorted_lyric_segmentation_count_df = pd.DataFrame(list(sorted_lyric_segmentation_count.items()))\n",
    "sorted_lyric_segmentation_count_df.columns = [\"character\", \"count\"]\n",
    "\n",
    "sorted_lyric_rm_stopwords_segmentation_count_df = pd.DataFrame(list(sorted_rm_stopwords_lyric_segmentation_count.items()))\n",
    "sorted_lyric_rm_stopwords_segmentation_count_df.columns = [\"character\", \"count\"]\n",
    "\n",
    "# export\n",
    "sorted_lyric_segmentation_count_df.to_csv (abs_path + \"//\" + file_name + \"_word_count.csv\", index = False, header=True, encoding=\"UTF-8\")\n",
    "sorted_lyric_rm_stopwords_segmentation_count_df.to_csv (abs_path + \"//\" + file_name + \"_rm_stopwords_word_count.csv\", index = False, header=True, encoding=\"UTF-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "636887f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output each individual word count for us songs\n",
    "\n",
    "file_name = \"us_song_lyrics_2020\"\n",
    "\n",
    "# flattten song lyric list to prepare for segmentation\n",
    "df = pd.read_csv(abs_path + \"//\" + file_name + \".csv\", encoding=\"UTF-8\", header=None)\n",
    "songs_to_segment = df.values.tolist()\n",
    "flattened_song_list_1 = ' '.join([data.lower() for song in songs_to_segment for data in song])\n",
    "\n",
    "flattened_song_list_2 = str(\"\\n\".join([flattened_song_list_1]))\n",
    "flattened_song_list_2 = flattened_song_list_2.replace(\"\\n\", \" \")\n",
    "\n",
    "# seperate contractions into two seperate words\n",
    "flattened_song_list_3 = contractions.fix(flattened_song_list_2) \n",
    "\n",
    "# input is a list of string sentences that need to be flatenned.\n",
    "lyric_segmentation = nltk.word_tokenize(flattened_song_list_3)\n",
    "\n",
    "# remove punctuation characters (will also remove any words that include a puncuation character)\n",
    "lyric_segmentation = [word for word in lyric_segmentation if word.isalpha()] \n",
    "\n",
    "# remove stop words\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "lyric_rm_stopwords_segmentation = [w for w in lyric_segmentation if not w in stop_words]\n",
    "\n",
    "\n",
    "# segment words\n",
    "lyric_pos = nltk.pos_tag(lyric_segmentation)\n",
    "lyric_rm_stopwords_pos = nltk.pos_tag(lyric_rm_stopwords_segmentation)\n",
    "\n",
    "\n",
    "# make it easier to categorize words in post by categorizing based on part of speech. Update- it turns out nltk's part of speech categorizer isn't the most accurate. For example the number of times that love is referenced in the feelings category is much less than without part of speech categorization. More investigation is needed to determine if it's worth using.\n",
    "def feelings(pos_tuple):\n",
    "    word, tag = pos_tuple\n",
    "    if tag.startswith((\"JJ\", \"RB\", \"VB\")):\n",
    "        return True\n",
    "    # return True\n",
    "lyric_feelings_segmentation = [word for word, tag in filter(\n",
    "    feelings,\n",
    "    lyric_rm_stopwords_pos\n",
    ")]\n",
    "\n",
    "def pronouns(pos_tuple):\n",
    "    word, tag = pos_tuple\n",
    "    if tag.startswith((\"PRP\")):\n",
    "        return True\n",
    "    elif word == \"i\":\n",
    "        return True\n",
    "lyric_pronouns_segmentation = [word for word, tag in filter(\n",
    "    pronouns,\n",
    "    lyric_pos\n",
    ")]\n",
    "\n",
    "def nouns(pos_tuple):\n",
    "    word, tag = pos_tuple\n",
    "    if tag.startswith((\"NN\")):\n",
    "        return True\n",
    "lyric_nouns_segmentation = [word for word, tag in filter(\n",
    "    nouns,\n",
    "    lyric_rm_stopwords_pos\n",
    ")]\n",
    "\n",
    "\n",
    "# calculate each word's word count\n",
    "def countOccurrence(a):\n",
    "  k = {}\n",
    "  for j in a:\n",
    "    if j in k:\n",
    "      k[j] +=1\n",
    "    else:\n",
    "      k[j] =1\n",
    "  return k\n",
    "\n",
    "# lyric_segmentation_count = countOccurrence(lyric_segmentation)\n",
    "# lyric_feelings_segmentation_count = countOccurrence(lyric_feelings)\n",
    "# lyric_pronouns_segmentation_count = countOccurrence(lyric_feelings)\n",
    "# lyric_segmentation_count_rm_stopwords = countOccurrence(lyric_rm_stopwords_segmentation)\n",
    "\n",
    "# frequency distributions\n",
    "frequency_distribution_all = nltk.FreqDist(lyric_segmentation)\n",
    "frequency_distribution_feelings = nltk.FreqDist(lyric_feelings_segmentation)\n",
    "frequency_distribution_pronouns = nltk.FreqDist(lyric_pronouns_segmentation)\n",
    "frequency_distribution_nouns = nltk.FreqDist(lyric_nouns_segmentation)\n",
    "frequency_distribution_rm_stopwords = nltk.FreqDist(lyric_rm_stopwords_segmentation)\n",
    "\n",
    "# make into dataframes\n",
    "frequency_distribution_all_df = pd.DataFrame(list(frequency_distribution_all.most_common())) # the nltk.FreqDist object has built in functions, like taking the most_common will sort the frequency distribution \n",
    "frequency_distribution_all_df.columns = [\"word\", \"count\"]\n",
    "\n",
    "frequency_distribution_feelings_df = pd.DataFrame(list(frequency_distribution_feelings.most_common()))\n",
    "frequency_distribution_feelings_df.columns = [\"word\", \"count\"]\n",
    "\n",
    "frequency_distribution_pronouns_df = pd.DataFrame(list(frequency_distribution_pronouns.most_common()))\n",
    "frequency_distribution_pronouns_df.columns = [\"word\", \"count\"]\n",
    "\n",
    "frequency_distribution_nouns_df = pd.DataFrame(list(frequency_distribution_nouns.most_common()))\n",
    "frequency_distribution_nouns_df.columns = [\"word\", \"count\"]\n",
    "\n",
    "frequency_distribution_rm_stopwords_df = pd.DataFrame(list(frequency_distribution_rm_stopwords.most_common()))\n",
    "frequency_distribution_rm_stopwords_df.columns = [\"word\", \"count\"]\n",
    "\n",
    "# output each category into csv\n",
    "frequency_distribution_all_df.to_csv (abs_path + \"//\" + file_name + \"_all_word_count.csv\", index = False, header=True, encoding=\"UTF-8\")\n",
    "frequency_distribution_feelings_df.to_csv (abs_path + \"//\" + file_name + \"_feelings_word_count.csv\", index = False, header=True, encoding=\"UTF-8\")\n",
    "frequency_distribution_pronouns_df.to_csv (abs_path + \"//\" + file_name + \"_pronouns_word_count.csv\", index = False, header=True, encoding=\"UTF-8\")\n",
    "frequency_distribution_nouns_df.to_csv (abs_path + \"//\" + file_name + \"_nouns_word_count.csv\", index = False, header=True, encoding=\"UTF-8\")\n",
    "frequency_distribution_rm_stopwords_df.to_csv (abs_path + \"//\" + file_name + \"_rm_stopwords_word_count.csv\", index = False, header=True, encoding=\"UTF-8\")\n",
    "\n",
    "# compile into an excel file\n",
    "def excel_export():\n",
    "    global tab\n",
    "    tab = \"compiled\"\n",
    "    all_files = glob.glob(os.path.join(abs_path + \"//\", (file_name + \"*.csv\"))) # Find all files that have a .csv extenstion and certain prefix\n",
    "    all_files.sort(key=os.path.getctime)\n",
    "\n",
    "    writer = pd.ExcelWriter(os.path.join(abs_path + \"//\" + file_name + \"_\" + tab + \".xlsx\"), engine='xlsxwriter')\n",
    "\n",
    "    for f in all_files:\n",
    "        df = pd.read_csv(f)\n",
    "        df.to_excel(writer, sheet_name=os.path.splitext(os.path.basename(f))[0][20:], index=False)\n",
    "    writer.save()\n",
    "    \n",
    "excel_export()\n",
    "\n",
    "# Manual frequency distribution count and sorting method\n",
    "# # sort word count\n",
    "# sorted_keys = sorted(lyric_segmentation_count, key=lyric_segmentation_count.get, reverse=True)\n",
    "# sorted_lyric_segmentation_count = {}\n",
    "# for w in sorted_keys:\n",
    "#     sorted_lyric_segmentation_count[w] = lyric_segmentation_count[w]\n",
    "\n",
    "# # sort word count\n",
    "# sorted_keys_rm_stopwords = sorted(lyric_segmentation_count_rm_stopwords, key=lyric_segmentation_count_rm_stopwords.get, reverse=True)\n",
    "# sorted_lyric_segmentation_rm_stopwords_count = {}\n",
    "# for w in sorted_keys_rm_stopwords:\n",
    "#     sorted_lyric_segmentation_rm_stopwords_count[w] = lyric_segmentation_count_rm_stopwords[w]\n",
    "\n",
    "# # convert dictionary to dataframe\n",
    "# sorted_lyric_segmentation_count_df = pd.DataFrame(list(sorted_lyric_segmentation_count.items()))\n",
    "# sorted_lyric_segmentation_count_df = sorted_lyric_segmentation_count_df.rename(columns={0: \"word\", 1: \"count\"})\n",
    "# frequency_distribution_all_df = pd.DataFrame(list(frequency_distribution_all.most_common()))\n",
    "# frequency_distribution_df = frequency_distribution_df.rename(columns={0: \"word\", 1: \"count\"})\n",
    "# sorted_lyric_segmentation_count_df.to_csv (abs_path + \"//\" + file_name + \"_word_count.csv\", index = False, header=True, encoding=\"UTF-8\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "71846abdb62651062b88435f48b8bd8fc0ed277535a4bc396e17a46db90d2c6d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.11 ('web_scraper')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
