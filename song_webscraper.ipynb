{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3aaa4efe-821f-4996-8803-8db8c12efcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import requests\n",
    "import shutil\n",
    "import glob\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import pywordseg as seg  # https://pypi.org/project/pywordseg/\n",
    "from TCSP import read_stopwords_list\n",
    "import nltk\n",
    "import contractions\n",
    "from nltk.corpus import stopwords \n",
    "#nltk.download('vader_lexicon')\n",
    "#nltk.download('popular')\n",
    "#%pip install xlsxwriter\n",
    "# %pip install tcsp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f91ab55f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['一',\n",
       " '一個',\n",
       " '一些',\n",
       " '一何',\n",
       " '一切',\n",
       " '一則',\n",
       " '一方面',\n",
       " '一旦',\n",
       " '一來',\n",
       " '一樣',\n",
       " '一種',\n",
       " '一般',\n",
       " '一轉眼',\n",
       " '七',\n",
       " '萬一',\n",
       " '三',\n",
       " '上',\n",
       " '上下',\n",
       " '下',\n",
       " '不',\n",
       " '不僅',\n",
       " '不但',\n",
       " '不光',\n",
       " '不單',\n",
       " '不只',\n",
       " '不外乎',\n",
       " '不如',\n",
       " '不妨',\n",
       " '不盡',\n",
       " '不盡然',\n",
       " '不得',\n",
       " '不怕',\n",
       " '不惟',\n",
       " '不成',\n",
       " '不拘',\n",
       " '不料',\n",
       " '不是',\n",
       " '不比',\n",
       " '不然',\n",
       " '不特',\n",
       " '不獨',\n",
       " '不管',\n",
       " '不至於',\n",
       " '不若',\n",
       " '不論',\n",
       " '不過',\n",
       " '不問',\n",
       " '與',\n",
       " '與其',\n",
       " '與其說',\n",
       " '與否',\n",
       " '與此同時',\n",
       " '且',\n",
       " '且不說',\n",
       " '且說',\n",
       " '兩者',\n",
       " '個',\n",
       " '個別',\n",
       " '中',\n",
       " '臨',\n",
       " '為',\n",
       " '為了',\n",
       " '為什麼',\n",
       " '為何',\n",
       " '為止',\n",
       " '為此',\n",
       " '為著',\n",
       " '乃',\n",
       " '乃至',\n",
       " '乃至於',\n",
       " '麼',\n",
       " '之',\n",
       " '之一',\n",
       " '之所以',\n",
       " '之類',\n",
       " '烏乎',\n",
       " '乎',\n",
       " '乘',\n",
       " '九',\n",
       " '也',\n",
       " '也好',\n",
       " '也罷',\n",
       " '了',\n",
       " '二',\n",
       " '二來',\n",
       " '於',\n",
       " '於是',\n",
       " '於是乎',\n",
       " '雲雲',\n",
       " '雲爾',\n",
       " '五',\n",
       " '些',\n",
       " '亦',\n",
       " '人',\n",
       " '人們',\n",
       " '人家',\n",
       " '什',\n",
       " '什麼',\n",
       " '什麼樣',\n",
       " '今',\n",
       " '介於',\n",
       " '仍',\n",
       " '仍舊',\n",
       " '從',\n",
       " '從此',\n",
       " '從而',\n",
       " '他',\n",
       " '他人',\n",
       " '他們',\n",
       " '他們們',\n",
       " '以',\n",
       " '以上',\n",
       " '以為',\n",
       " '以便',\n",
       " '以免',\n",
       " '以及',\n",
       " '以故',\n",
       " '以期',\n",
       " '以來',\n",
       " '以至',\n",
       " '以至於',\n",
       " '以致',\n",
       " '們',\n",
       " '任',\n",
       " '任何',\n",
       " '任憑',\n",
       " '會',\n",
       " '似的',\n",
       " '但',\n",
       " '但凡',\n",
       " '但是',\n",
       " '何',\n",
       " '何以',\n",
       " '何況',\n",
       " '何處',\n",
       " '何時',\n",
       " '余外',\n",
       " '作為',\n",
       " '你',\n",
       " '你們',\n",
       " '使',\n",
       " '使得',\n",
       " '例如',\n",
       " '依',\n",
       " '依據',\n",
       " '依照',\n",
       " '便於',\n",
       " '俺',\n",
       " '俺們',\n",
       " '倘',\n",
       " '倘使',\n",
       " '倘或',\n",
       " '倘然',\n",
       " '倘若',\n",
       " '借',\n",
       " '借儻然',\n",
       " '假使',\n",
       " '假如',\n",
       " '假若',\n",
       " '做',\n",
       " '像',\n",
       " '兒',\n",
       " '先不先',\n",
       " '光',\n",
       " '光是',\n",
       " '全體',\n",
       " '全部',\n",
       " '八',\n",
       " '六',\n",
       " '兮',\n",
       " '共',\n",
       " '關於',\n",
       " '關於具體地說',\n",
       " '其',\n",
       " '其一',\n",
       " '其中',\n",
       " '其二',\n",
       " '其他',\n",
       " '其餘',\n",
       " '其它',\n",
       " '其次',\n",
       " '具體地說',\n",
       " '具體說來',\n",
       " '兼之',\n",
       " '內',\n",
       " '再',\n",
       " '再其次',\n",
       " '再則',\n",
       " '再有',\n",
       " '再者',\n",
       " '再者說',\n",
       " '再說',\n",
       " '冒',\n",
       " '衝',\n",
       " '況且',\n",
       " '幾',\n",
       " '幾時',\n",
       " '凡',\n",
       " '凡是',\n",
       " '憑',\n",
       " '憑借',\n",
       " '出於',\n",
       " '出來',\n",
       " '分',\n",
       " '分別',\n",
       " '則',\n",
       " '則甚',\n",
       " '別',\n",
       " '別人',\n",
       " '別處',\n",
       " '別是',\n",
       " '別的',\n",
       " '別管',\n",
       " '別說',\n",
       " '到',\n",
       " '前後',\n",
       " '前此',\n",
       " '前者',\n",
       " '加之',\n",
       " '加以',\n",
       " '區',\n",
       " '即',\n",
       " '即令',\n",
       " '即使',\n",
       " '即便',\n",
       " '即如',\n",
       " '即或',\n",
       " '即若',\n",
       " '卻',\n",
       " '去',\n",
       " '又',\n",
       " '又及',\n",
       " '及',\n",
       " '及其',\n",
       " '及至',\n",
       " '反之',\n",
       " '反而',\n",
       " '反過來',\n",
       " '反過來說',\n",
       " '受到',\n",
       " '另',\n",
       " '另一方面',\n",
       " '另外',\n",
       " '另悉',\n",
       " '只',\n",
       " '只當',\n",
       " '只怕',\n",
       " '只是',\n",
       " '只有',\n",
       " '只消',\n",
       " '只要',\n",
       " '只限',\n",
       " '叫',\n",
       " '叮咚',\n",
       " '可',\n",
       " '可以',\n",
       " '可是',\n",
       " '可見',\n",
       " '各',\n",
       " '各個',\n",
       " '各位',\n",
       " '各種',\n",
       " '各自',\n",
       " '同',\n",
       " '同時',\n",
       " '後',\n",
       " '後者',\n",
       " '向',\n",
       " '向使',\n",
       " '向著',\n",
       " '嚇',\n",
       " '嗎',\n",
       " '否則',\n",
       " '吧',\n",
       " '吧噠',\n",
       " '含',\n",
       " '吱',\n",
       " '呀',\n",
       " '呃',\n",
       " '嘔',\n",
       " '唄',\n",
       " '嗚',\n",
       " '嗚呼',\n",
       " '呢',\n",
       " '呵',\n",
       " '呵呵',\n",
       " '呸',\n",
       " '呼哧',\n",
       " '咋',\n",
       " '和',\n",
       " '咚',\n",
       " '咦',\n",
       " '咧',\n",
       " '咱',\n",
       " '咱們',\n",
       " '咳',\n",
       " '哇',\n",
       " '哈',\n",
       " '哈哈',\n",
       " '哉',\n",
       " '哎',\n",
       " '哎呀',\n",
       " '哎喲',\n",
       " '嘩',\n",
       " '喲',\n",
       " '哦',\n",
       " '哩',\n",
       " '哪',\n",
       " '哪個',\n",
       " '哪些',\n",
       " '哪兒',\n",
       " '哪天',\n",
       " '哪年',\n",
       " '哪怕',\n",
       " '哪樣',\n",
       " '哪邊',\n",
       " '哪裡',\n",
       " '哼',\n",
       " '哼唷',\n",
       " '唉',\n",
       " '唯有',\n",
       " '啊',\n",
       " '啐',\n",
       " '啥',\n",
       " '啦',\n",
       " '啪達',\n",
       " '啷當',\n",
       " '餵',\n",
       " '喏',\n",
       " '喔唷',\n",
       " '嘍',\n",
       " '嗡',\n",
       " '嗡嗡',\n",
       " '嗬',\n",
       " '嗯',\n",
       " '噯',\n",
       " '嘎',\n",
       " '嘎登',\n",
       " '噓',\n",
       " '嘛',\n",
       " '嘻',\n",
       " '嘿',\n",
       " '嘿嘿',\n",
       " '四',\n",
       " '因',\n",
       " '因為',\n",
       " '因了',\n",
       " '因此',\n",
       " '因著',\n",
       " '因而',\n",
       " '固然',\n",
       " '在',\n",
       " '在下',\n",
       " '在於',\n",
       " '地',\n",
       " '基於',\n",
       " '處在',\n",
       " '多',\n",
       " '多麼',\n",
       " '多少',\n",
       " '大',\n",
       " '大家',\n",
       " '她',\n",
       " '她們',\n",
       " '好',\n",
       " '如',\n",
       " '如上',\n",
       " '如上所述',\n",
       " '如下',\n",
       " '如何',\n",
       " '如其',\n",
       " '如同',\n",
       " '如是',\n",
       " '如果',\n",
       " '如此',\n",
       " '如若',\n",
       " '始而',\n",
       " '孰料',\n",
       " '孰知',\n",
       " '寧',\n",
       " '寧可',\n",
       " '寧願',\n",
       " '寧肯',\n",
       " '它',\n",
       " '它們',\n",
       " '對',\n",
       " '對於',\n",
       " '對待',\n",
       " '對方',\n",
       " '對比',\n",
       " '將',\n",
       " '小',\n",
       " '爾',\n",
       " '爾後',\n",
       " '爾爾',\n",
       " '尚且',\n",
       " '就',\n",
       " '就是',\n",
       " '就是了',\n",
       " '就是說',\n",
       " '就算',\n",
       " '就要',\n",
       " '盡',\n",
       " '儘管',\n",
       " '儘管如此',\n",
       " '豈但',\n",
       " '己',\n",
       " '已',\n",
       " '已矣',\n",
       " '巴',\n",
       " '巴巴',\n",
       " '年',\n",
       " '並',\n",
       " '並且',\n",
       " '庶乎',\n",
       " '庶幾',\n",
       " '開外',\n",
       " '開始',\n",
       " '歸',\n",
       " '歸齊',\n",
       " '當',\n",
       " '當地',\n",
       " '當然',\n",
       " '當著',\n",
       " '彼',\n",
       " '彼時',\n",
       " '彼此',\n",
       " '往',\n",
       " '待',\n",
       " '很',\n",
       " '得',\n",
       " '得了',\n",
       " '怎',\n",
       " '怎麼',\n",
       " '怎麼辦',\n",
       " '怎麼樣',\n",
       " '怎奈',\n",
       " '怎樣',\n",
       " '總之',\n",
       " '總的來看',\n",
       " '總的來說',\n",
       " '總的說來',\n",
       " '總而言之',\n",
       " '恰恰相反',\n",
       " '您',\n",
       " '惟其',\n",
       " '慢說',\n",
       " '我',\n",
       " '我們',\n",
       " '或',\n",
       " '或則',\n",
       " '或是',\n",
       " '或曰',\n",
       " '或者',\n",
       " '截至',\n",
       " '所',\n",
       " '所以',\n",
       " '所在',\n",
       " '所幸',\n",
       " '所有',\n",
       " '才',\n",
       " '才能',\n",
       " '打',\n",
       " '打從',\n",
       " '把',\n",
       " '抑或',\n",
       " '拿',\n",
       " '按',\n",
       " '按照',\n",
       " '換句話說',\n",
       " '換言之',\n",
       " '據',\n",
       " '據此',\n",
       " '接著',\n",
       " '故',\n",
       " '故此',\n",
       " '故而',\n",
       " '旁人',\n",
       " '無',\n",
       " '無寧',\n",
       " '無論',\n",
       " '既',\n",
       " '既往',\n",
       " '既是',\n",
       " '既然',\n",
       " '日',\n",
       " '時',\n",
       " '時候',\n",
       " '是',\n",
       " '是以',\n",
       " '是的',\n",
       " '更',\n",
       " '曾',\n",
       " '替',\n",
       " '替代',\n",
       " '最',\n",
       " '月',\n",
       " '有',\n",
       " '有些',\n",
       " '有關',\n",
       " '有及',\n",
       " '有時',\n",
       " '有的',\n",
       " '望',\n",
       " '朝',\n",
       " '朝著',\n",
       " '本',\n",
       " '本人',\n",
       " '本地',\n",
       " '本著',\n",
       " '本身',\n",
       " '來',\n",
       " '來著',\n",
       " '來自',\n",
       " '來說',\n",
       " '極了',\n",
       " '果然',\n",
       " '果真',\n",
       " '某',\n",
       " '某個',\n",
       " '某些',\n",
       " '某某',\n",
       " '根據',\n",
       " '歟',\n",
       " '正值',\n",
       " '正如',\n",
       " '正巧',\n",
       " '正是',\n",
       " '此',\n",
       " '此地',\n",
       " '此處',\n",
       " '此外',\n",
       " '此時',\n",
       " '此次',\n",
       " '此間',\n",
       " '毋寧',\n",
       " '每',\n",
       " '每當',\n",
       " '比',\n",
       " '比及',\n",
       " '比如',\n",
       " '比方',\n",
       " '沒奈何',\n",
       " '沿',\n",
       " '沿著',\n",
       " '漫說',\n",
       " '點',\n",
       " '焉',\n",
       " '然則',\n",
       " '然後',\n",
       " '然而',\n",
       " '照',\n",
       " '照著',\n",
       " '猶且',\n",
       " '猶自',\n",
       " '甚且',\n",
       " '甚麼',\n",
       " '甚或',\n",
       " '甚而',\n",
       " '甚至',\n",
       " '甚至於',\n",
       " '用',\n",
       " '用來',\n",
       " '由',\n",
       " '由於',\n",
       " '由是',\n",
       " '由此',\n",
       " '由此可見',\n",
       " '的',\n",
       " '的確',\n",
       " '的話',\n",
       " '直到',\n",
       " '相對而言',\n",
       " '省得',\n",
       " '看',\n",
       " '眨眼',\n",
       " '著',\n",
       " '著呢',\n",
       " '矣',\n",
       " '矣乎',\n",
       " '矣哉',\n",
       " '離',\n",
       " '秒',\n",
       " '稱',\n",
       " '竟而',\n",
       " '第',\n",
       " '等',\n",
       " '等到',\n",
       " '等等',\n",
       " '簡言之',\n",
       " '管',\n",
       " '類如',\n",
       " '緊接著',\n",
       " '縱',\n",
       " '縱令',\n",
       " '縱使',\n",
       " '縱然',\n",
       " '經',\n",
       " '經過',\n",
       " '結果',\n",
       " '給',\n",
       " '繼之',\n",
       " '繼後',\n",
       " '繼而',\n",
       " '綜上所述',\n",
       " '罷了',\n",
       " '者',\n",
       " '而',\n",
       " '而且',\n",
       " '而況',\n",
       " '而後',\n",
       " '而外',\n",
       " '而已',\n",
       " '而是',\n",
       " '而言',\n",
       " '能',\n",
       " '能否',\n",
       " '騰',\n",
       " '自',\n",
       " '自個兒',\n",
       " '自從',\n",
       " '自各兒',\n",
       " '自後',\n",
       " '自家',\n",
       " '自己',\n",
       " '自打',\n",
       " '自身',\n",
       " '至',\n",
       " '至於',\n",
       " '至今',\n",
       " '至若',\n",
       " '致',\n",
       " '般的',\n",
       " '若',\n",
       " '若夫',\n",
       " '若是',\n",
       " '若果',\n",
       " '若非',\n",
       " '莫不然',\n",
       " '莫如',\n",
       " '莫若',\n",
       " '雖',\n",
       " '雖則',\n",
       " '雖然',\n",
       " '雖說',\n",
       " '被',\n",
       " '要',\n",
       " '要不',\n",
       " '要不是',\n",
       " '要不然',\n",
       " '要麼',\n",
       " '要是',\n",
       " '譬喻',\n",
       " '譬如',\n",
       " '讓',\n",
       " '許多',\n",
       " '論',\n",
       " '設使',\n",
       " '設或',\n",
       " '設若',\n",
       " '誠如',\n",
       " '誠然',\n",
       " '該',\n",
       " '說',\n",
       " '說來',\n",
       " '請',\n",
       " '諸',\n",
       " '諸位',\n",
       " '諸如',\n",
       " '誰',\n",
       " '誰人',\n",
       " '誰料',\n",
       " '誰知',\n",
       " '賊死',\n",
       " '賴以',\n",
       " '趕',\n",
       " '起',\n",
       " '起見',\n",
       " '趁',\n",
       " '趁著',\n",
       " '越是',\n",
       " '距',\n",
       " '跟',\n",
       " '較',\n",
       " '較之',\n",
       " '邊',\n",
       " '過',\n",
       " '還',\n",
       " '還是',\n",
       " '還有',\n",
       " '還要',\n",
       " '這',\n",
       " '這一來',\n",
       " '這個',\n",
       " '這麼',\n",
       " '這麼些',\n",
       " '這麼樣',\n",
       " '這麼點兒',\n",
       " '這些',\n",
       " '這會兒',\n",
       " '這兒',\n",
       " '這就是說',\n",
       " '這時',\n",
       " '這樣',\n",
       " '這次',\n",
       " '這般',\n",
       " '這邊',\n",
       " '這裡',\n",
       " '進而',\n",
       " '連',\n",
       " '連同',\n",
       " '逐步',\n",
       " '通過',\n",
       " '遵循',\n",
       " '遵照',\n",
       " '那',\n",
       " '那個',\n",
       " '那麼',\n",
       " '那麼些',\n",
       " '那麼樣',\n",
       " '那些',\n",
       " '那會兒',\n",
       " '那兒',\n",
       " '那時',\n",
       " '那樣',\n",
       " '那般',\n",
       " '那邊',\n",
       " '那裡',\n",
       " '都',\n",
       " '鄙人',\n",
       " '鑒於',\n",
       " '針對',\n",
       " '阿',\n",
       " '除',\n",
       " '除了',\n",
       " '除外',\n",
       " '除開',\n",
       " '除此之外',\n",
       " '除非',\n",
       " '隨',\n",
       " '隨後',\n",
       " '隨時',\n",
       " '隨著',\n",
       " '難道說',\n",
       " '零',\n",
       " '非',\n",
       " '非但',\n",
       " '非徒',\n",
       " '非特',\n",
       " '非獨',\n",
       " '靠',\n",
       " '順',\n",
       " '順著',\n",
       " '首先',\n",
       " '︿',\n",
       " '！',\n",
       " '＃',\n",
       " '＄',\n",
       " '％',\n",
       " '＆',\n",
       " '（',\n",
       " '）',\n",
       " '＊',\n",
       " '＋',\n",
       " '，',\n",
       " '０',\n",
       " '１',\n",
       " '２',\n",
       " '３',\n",
       " '４',\n",
       " '５',\n",
       " '６',\n",
       " '７',\n",
       " '８',\n",
       " '９',\n",
       " '：',\n",
       " '；',\n",
       " '＜',\n",
       " '＞',\n",
       " '？',\n",
       " '＠',\n",
       " '［',\n",
       " '］',\n",
       " '｛',\n",
       " '｜',\n",
       " '｝',\n",
       " '～',\n",
       " '￥',\n",
       " 'i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " '的',\n",
       " '了',\n",
       " '和',\n",
       " '是',\n",
       " '就',\n",
       " '都',\n",
       " '而',\n",
       " '及',\n",
       " '與',\n",
       " '著',\n",
       " '或',\n",
       " '一個',\n",
       " '沒有',\n",
       " '我們',\n",
       " '你們',\n",
       " '妳們',\n",
       " '他們',\n",
       " '她們',\n",
       " '是否',\n",
       " '。',\n",
       " ',',\n",
       " '「',\n",
       " '」',\n",
       " '、',\n",
       " '‧',\n",
       " '《',\n",
       " '》',\n",
       " '〈',\n",
       " '〉',\n",
       " '——',\n",
       " '—',\n",
       " '～',\n",
       " '【',\n",
       " ...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_stopwords_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e547b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading channels: done\n",
      "# Name                       Version           Build  Channel             \n",
      "pandas                        0.20.3  py27h2a3d8ca_2  pkgs/main           \n",
      "pandas                        0.20.3  py35h5e2f206_2  pkgs/main           \n",
      "pandas                        0.20.3  py36hd6655d8_2  pkgs/main           \n",
      "pandas                        0.21.0  py27h860f240_1  pkgs/main           \n",
      "pandas                        0.21.0  py27h8d379d7_0  pkgs/main           \n",
      "pandas                        0.21.0  py35h4431f20_1  pkgs/main           \n",
      "pandas                        0.21.0  py35h4af63c7_0  pkgs/main           \n",
      "pandas                        0.21.0  py36hd8e0503_0  pkgs/main           \n",
      "pandas                        0.21.0  py36hfed917e_1  pkgs/main           \n",
      "pandas                        0.21.1  py27h1cb45b9_0  pkgs/main           \n",
      "pandas                        0.21.1  py35h2c08c6b_0  pkgs/main           \n",
      "pandas                        0.21.1  py36h2c08c6b_0  pkgs/main           \n",
      "pandas                        0.22.0  py27h0a44026_0  pkgs/main           \n",
      "pandas                        0.22.0  py35h0a44026_0  pkgs/main           \n",
      "pandas                        0.22.0  py36h0a44026_0  pkgs/main           \n",
      "pandas                        0.23.0  py27h1702cab_0  pkgs/main           \n",
      "pandas                        0.23.0  py35h1702cab_0  pkgs/main           \n",
      "pandas                        0.23.0  py36h1702cab_0  pkgs/main           \n",
      "pandas                        0.23.1  py27h1702cab_0  pkgs/main           \n",
      "pandas                        0.23.1  py35h1702cab_0  pkgs/main           \n",
      "pandas                        0.23.1  py36h1702cab_0  pkgs/main           \n",
      "pandas                        0.23.2  py27h6440ff4_0  pkgs/main           \n",
      "pandas                        0.23.2  py36h6440ff4_0  pkgs/main           \n",
      "pandas                        0.23.2  py37h6440ff4_0  pkgs/main           \n",
      "pandas                        0.23.3  py27h6440ff4_0  pkgs/main           \n",
      "pandas                        0.23.3  py35h6440ff4_0  pkgs/main           \n",
      "pandas                        0.23.3  py36h6440ff4_0  pkgs/main           \n",
      "pandas                        0.23.3  py37h6440ff4_0  pkgs/main           \n",
      "pandas                        0.23.4  py27h6440ff4_0  pkgs/main           \n",
      "pandas                        0.23.4  py35h6440ff4_0  pkgs/main           \n",
      "pandas                        0.23.4  py36h6440ff4_0  pkgs/main           \n",
      "pandas                        0.23.4  py37h6440ff4_0  pkgs/main           \n",
      "pandas                        0.24.0  py27h0a44026_0  pkgs/main           \n",
      "pandas                        0.24.0  py36h0a44026_0  pkgs/main           \n",
      "pandas                        0.24.0  py37h0a44026_0  pkgs/main           \n",
      "pandas                        0.24.1  py27h0a44026_0  pkgs/main           \n",
      "pandas                        0.24.1  py36h0a44026_0  pkgs/main           \n",
      "pandas                        0.24.1  py37h0a44026_0  pkgs/main           \n",
      "pandas                        0.24.2  py27h0a44026_0  pkgs/main           \n",
      "pandas                        0.24.2  py36h0a44026_0  pkgs/main           \n",
      "pandas                        0.24.2  py37h0a44026_0  pkgs/main           \n",
      "pandas                        0.25.0  py36h0a44026_0  pkgs/main           \n",
      "pandas                        0.25.0  py37h0a44026_0  pkgs/main           \n",
      "pandas                        0.25.1  py36h0a44026_0  pkgs/main           \n",
      "pandas                        0.25.1  py37h0a44026_0  pkgs/main           \n",
      "pandas                        0.25.2  py36h0a44026_0  pkgs/main           \n",
      "pandas                        0.25.2  py37h0a44026_0  pkgs/main           \n",
      "pandas                        0.25.3  py36h0a44026_0  pkgs/main           \n",
      "pandas                        0.25.3  py37h0a44026_0  pkgs/main           \n",
      "pandas                        0.25.3  py38h0a44026_0  pkgs/main           \n",
      "pandas                         1.0.0  py36h6c726b0_0  pkgs/main           \n",
      "pandas                         1.0.0  py37h6c726b0_0  pkgs/main           \n",
      "pandas                         1.0.0  py38h6c726b0_0  pkgs/main           \n",
      "pandas                         1.0.1  py36h6c726b0_0  pkgs/main           \n",
      "pandas                         1.0.1  py37h6c726b0_0  pkgs/main           \n",
      "pandas                         1.0.1  py38h6c726b0_0  pkgs/main           \n",
      "pandas                         1.0.2  py36h6c726b0_0  pkgs/main           \n",
      "pandas                         1.0.2  py37h6c726b0_0  pkgs/main           \n",
      "pandas                         1.0.2  py38h6c726b0_0  pkgs/main           \n",
      "pandas                         1.0.3  py36h6c726b0_0  pkgs/main           \n",
      "pandas                         1.0.3  py37h6c726b0_0  pkgs/main           \n",
      "pandas                         1.0.3  py38h6c726b0_0  pkgs/main           \n",
      "pandas                         1.0.4  py36h959d312_0  pkgs/main           \n",
      "pandas                         1.0.4  py37h959d312_0  pkgs/main           \n",
      "pandas                         1.0.4  py38h959d312_0  pkgs/main           \n",
      "pandas                         1.0.5  py36h959d312_0  pkgs/main           \n",
      "pandas                         1.0.5  py37h959d312_0  pkgs/main           \n",
      "pandas                         1.0.5  py38h959d312_0  pkgs/main           \n",
      "pandas                         1.1.0  py36hb1e8313_0  pkgs/main           \n",
      "pandas                         1.1.0  py37hb1e8313_0  pkgs/main           \n",
      "pandas                         1.1.0  py38hb1e8313_0  pkgs/main           \n",
      "pandas                         1.1.1  py36hb1e8313_0  pkgs/main           \n",
      "pandas                         1.1.1  py37hb1e8313_0  pkgs/main           \n",
      "pandas                         1.1.1  py38hb1e8313_0  pkgs/main           \n",
      "pandas                         1.1.2  py36hb1e8313_0  pkgs/main           \n",
      "pandas                         1.1.2  py37hb1e8313_0  pkgs/main           \n",
      "pandas                         1.1.2  py38hb1e8313_0  pkgs/main           \n",
      "pandas                         1.1.3  py36hb1e8313_0  pkgs/main           \n",
      "pandas                         1.1.3  py37hb1e8313_0  pkgs/main           \n",
      "pandas                         1.1.3  py38hb1e8313_0  pkgs/main           \n",
      "pandas                         1.1.3  py39hb2f4e1b_0  pkgs/main           \n",
      "pandas                         1.1.5  py36h23ab428_0  pkgs/main           \n",
      "pandas                         1.1.5  py36hb2f4e1b_0  pkgs/main           \n",
      "pandas                         1.1.5  py37h23ab428_0  pkgs/main           \n",
      "pandas                         1.1.5  py37hb2f4e1b_0  pkgs/main           \n",
      "pandas                         1.1.5  py38h23ab428_0  pkgs/main           \n",
      "pandas                         1.1.5  py38hb2f4e1b_0  pkgs/main           \n",
      "pandas                         1.1.5  py39hb2f4e1b_0  pkgs/main           \n",
      "pandas                         1.2.0  py37hb2f4e1b_0  pkgs/main           \n",
      "pandas                         1.2.0  py38hb2f4e1b_0  pkgs/main           \n",
      "pandas                         1.2.0  py39hb2f4e1b_0  pkgs/main           \n",
      "pandas                         1.2.1  py37hb2f4e1b_0  pkgs/main           \n",
      "pandas                         1.2.1  py38hb2f4e1b_0  pkgs/main           \n",
      "pandas                         1.2.1  py39hb2f4e1b_0  pkgs/main           \n",
      "pandas                         1.2.2  py37hb2f4e1b_0  pkgs/main           \n",
      "pandas                         1.2.2  py38hb2f4e1b_0  pkgs/main           \n",
      "pandas                         1.2.2  py39hb2f4e1b_0  pkgs/main           \n",
      "pandas                         1.2.3  py37hb2f4e1b_0  pkgs/main           \n",
      "pandas                         1.2.3  py38hb2f4e1b_0  pkgs/main           \n",
      "pandas                         1.2.3  py39hb2f4e1b_0  pkgs/main           \n",
      "pandas                         1.2.4  py37h23ab428_0  pkgs/main           \n",
      "pandas                         1.2.4  py37hb2f4e1b_0  pkgs/main           \n",
      "pandas                         1.2.4  py38h23ab428_0  pkgs/main           \n",
      "pandas                         1.2.4  py38hb2f4e1b_0  pkgs/main           \n",
      "pandas                         1.2.4  py39h23ab428_0  pkgs/main           \n",
      "pandas                         1.2.4  py39hb2f4e1b_0  pkgs/main           \n",
      "pandas                         1.2.5  py37h23ab428_0  pkgs/main           \n",
      "pandas                         1.2.5  py38h23ab428_0  pkgs/main           \n",
      "pandas                         1.2.5  py39h23ab428_0  pkgs/main           \n",
      "pandas                         1.3.0  py37h23ab428_0  pkgs/main           \n",
      "pandas                         1.3.0  py38h23ab428_0  pkgs/main           \n",
      "pandas                         1.3.0  py39h23ab428_0  pkgs/main           \n",
      "pandas                         1.3.1  py37h5008ddb_0  pkgs/main           \n",
      "pandas                         1.3.1  py38h5008ddb_0  pkgs/main           \n",
      "pandas                         1.3.1  py39h5008ddb_0  pkgs/main           \n",
      "pandas                         1.3.2  py37h5008ddb_0  pkgs/main           \n",
      "pandas                         1.3.2  py38h5008ddb_0  pkgs/main           \n",
      "pandas                         1.3.2  py39h5008ddb_0  pkgs/main           \n",
      "pandas                         1.3.3  py37h5008ddb_0  pkgs/main           \n",
      "pandas                         1.3.3  py38h5008ddb_0  pkgs/main           \n",
      "pandas                         1.3.3  py39h5008ddb_0  pkgs/main           \n",
      "pandas                         1.3.4  py37h743cdd8_0  pkgs/main           \n",
      "pandas                         1.3.4  py38h743cdd8_0  pkgs/main           \n",
      "pandas                         1.3.4  py39h743cdd8_0  pkgs/main           \n",
      "pandas                         1.3.5 py310hc081a56_0  pkgs/main           \n",
      "pandas                         1.3.5  py37h743cdd8_0  pkgs/main           \n",
      "pandas                         1.3.5  py38h743cdd8_0  pkgs/main           \n",
      "pandas                         1.3.5  py39h743cdd8_0  pkgs/main           \n",
      "pandas                         1.4.1 py310he9d5cce_0  pkgs/main           \n",
      "pandas                         1.4.1 py310he9d5cce_1  pkgs/main           \n",
      "pandas                         1.4.1  py38he9d5cce_0  pkgs/main           \n",
      "pandas                         1.4.1  py38he9d5cce_1  pkgs/main           \n",
      "pandas                         1.4.1  py39he9d5cce_0  pkgs/main           \n",
      "pandas                         1.4.1  py39he9d5cce_1  pkgs/main           \n",
      "pandas                         1.4.2 py310he9d5cce_0  pkgs/main           \n",
      "pandas                         1.4.2  py38he9d5cce_0  pkgs/main           \n",
      "pandas                         1.4.2  py39he9d5cce_0  pkgs/main           \n"
     ]
    }
   ],
   "source": [
    "!conda search pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d462049",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'stopwords-zht.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/ericdao/Documents/local_code/webscraper/song_webscraper.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ericdao/Documents/local_code/webscraper/song_webscraper.ipynb#ch0000097?line=0'>1</a>\u001b[0m TCSP\u001b[39m.\u001b[39;49mread_stopwords_list()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/web_scraper/lib/python3.9/site-packages/TCSP/__init__.py:5\u001b[0m, in \u001b[0;36mread_stopwords_list\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/site-packages/TCSP/__init__.py?line=1'>2</a>\u001b[0m stop_words \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/site-packages/TCSP/__init__.py?line=3'>4</a>\u001b[0m \u001b[39m# reading the custom stopwords text file\u001b[39;00m\n\u001b[0;32m----> <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/site-packages/TCSP/__init__.py?line=4'>5</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mstopwords-zht.txt\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m ) \u001b[39mas\u001b[39;00m f :\n\u001b[1;32m      <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/site-packages/TCSP/__init__.py?line=5'>6</a>\u001b[0m     lines \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mreadlines()\n\u001b[1;32m      <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/site-packages/TCSP/__init__.py?line=6'>7</a>\u001b[0m     \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m lines:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'stopwords-zht.txt'"
     ]
    }
   ],
   "source": [
    "TCSP.read_stopwords_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c011473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package              Version\n",
      "-------------------- ---------\n",
      "anyascii             0.3.1\n",
      "appnope              0.1.2\n",
      "argon2-cffi          21.3.0\n",
      "argon2-cffi-bindings 21.2.0\n",
      "asttokens            2.0.5\n",
      "attrs                21.4.0\n",
      "backcall             0.2.0\n",
      "beautifulsoup4       4.10.0\n",
      "bleach               4.1.0\n",
      "Bottleneck           1.3.4\n",
      "brotlipy             0.7.0\n",
      "certifi              2021.10.8\n",
      "cffi                 1.15.0\n",
      "charset-normalizer   2.0.12\n",
      "click                8.1.3\n",
      "contractions         0.1.72\n",
      "cryptography         36.0.0\n",
      "debugpy              1.5.1\n",
      "decorator            5.1.1\n",
      "defusedxml           0.7.1\n",
      "entrypoints          0.3\n",
      "executing            0.8.3\n",
      "h5py                 3.6.0\n",
      "idna                 3.3\n",
      "importlib-metadata   4.11.3\n",
      "ipykernel            6.9.1\n",
      "ipython              8.2.0\n",
      "ipython-genutils     0.2.0\n",
      "ipywidgets           7.6.5\n",
      "jedi                 0.18.1\n",
      "Jinja2               3.0.3\n",
      "joblib               1.1.0\n",
      "jsonschema           3.2.0\n",
      "jupyter              1.0.0\n",
      "jupyter-client       7.1.2\n",
      "jupyter-console      6.4.3\n",
      "jupyter-core         4.9.2\n",
      "jupyterlab-pygments  0.1.2\n",
      "jupyterlab-widgets   1.0.0\n",
      "MarkupSafe           2.0.1\n",
      "matplotlib-inline    0.1.2\n",
      "mistune              0.8.4\n",
      "mkl-fft              1.3.1\n",
      "mkl-random           1.2.2\n",
      "mkl-service          2.4.0\n",
      "nbclient             0.5.11\n",
      "nbconvert            6.3.0\n",
      "nbformat             5.1.3\n",
      "nest-asyncio         1.5.1\n",
      "nltk                 3.7\n",
      "notebook             6.4.8\n",
      "numexpr              2.8.1\n",
      "numpy                1.21.2\n",
      "overrides            1.9\n",
      "packaging            21.3\n",
      "pandas               1.4.1\n",
      "pandocfilters        1.5.0\n",
      "parso                0.8.3\n",
      "pexpect              4.8.0\n",
      "pickleshare          0.7.5\n",
      "pip                  21.2.4\n",
      "prometheus-client    0.13.1\n",
      "prompt-toolkit       3.0.20\n",
      "ptyprocess           0.7.0\n",
      "pure-eval            0.2.2\n",
      "pyahocorasick        1.4.4\n",
      "pycparser            2.21\n",
      "Pygments             2.11.2\n",
      "pyOpenSSL            22.0.0\n",
      "pyparsing            3.0.4\n",
      "pyrsistent           0.18.0\n",
      "PySocks              1.7.1\n",
      "python-dateutil      2.8.2\n",
      "pytz                 2021.3\n",
      "pywordseg            0.1.4\n",
      "pyzmq                22.3.0\n",
      "qtconsole            5.3.0\n",
      "QtPy                 2.0.1\n",
      "regex                2022.4.24\n",
      "requests             2.27.1\n",
      "selenium             3.141.0\n",
      "Send2Trash           1.8.0\n",
      "setuptools           58.0.4\n",
      "sip                  4.19.13\n",
      "six                  1.16.0\n",
      "soupsieve            2.3.1\n",
      "stack-data           0.2.0\n",
      "TCSP                 0.0.3\n",
      "terminado            0.13.1\n",
      "testpath             0.5.0\n",
      "textsearch           0.0.21\n",
      "torch                1.11.0\n",
      "tornado              6.1\n",
      "tqdm                 4.64.0\n",
      "traitlets            5.1.1\n",
      "typing_extensions    4.1.1\n",
      "urllib3              1.26.8\n",
      "wcwidth              0.2.5\n",
      "webencodings         0.5.1\n",
      "wheel                0.37.1\n",
      "widgetsnbextension   3.5.2\n",
      "XlsxWriter           3.0.3\n",
      "zipp                 3.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28126295",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9ed076",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63592a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://realpython.com/python-nltk-sentiment-analysis/\n",
    "# https://machinelearningmastery.com/clean-text-machine-learning-python/\n",
    "# https://www.guru99.com/pos-tagging-chunking-nltk.html\n",
    "# https://www.nltk.org/api/nltk.tokenize.html\n",
    "# https://github.com/bryanchw/Traditional-Chinese-Stopwords-and-Punctuations-Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51704606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ericdao/Documents/local_code/webscraper'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c337d8be",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'stopwords-zht.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/ericdao/Documents/local_code/webscraper/song_webscraper.ipynb Cell 18'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ericdao/Documents/local_code/webscraper/song_webscraper.ipynb#ch0000140?line=0'>1</a>\u001b[0m read_stopwords_list()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/web_scraper/lib/python3.9/site-packages/TCSP/__init__.py:5\u001b[0m, in \u001b[0;36mread_stopwords_list\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/site-packages/TCSP/__init__.py?line=1'>2</a>\u001b[0m stop_words \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/site-packages/TCSP/__init__.py?line=3'>4</a>\u001b[0m \u001b[39m# reading the custom stopwords text file\u001b[39;00m\n\u001b[0;32m----> <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/site-packages/TCSP/__init__.py?line=4'>5</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mstopwords-zht.txt\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m ) \u001b[39mas\u001b[39;00m f :\n\u001b[1;32m      <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/site-packages/TCSP/__init__.py?line=5'>6</a>\u001b[0m     lines \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mreadlines()\n\u001b[1;32m      <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/site-packages/TCSP/__init__.py?line=6'>7</a>\u001b[0m     \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m lines:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'stopwords-zht.txt'"
     ]
    }
   ],
   "source": [
    "read_stopwords_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aead584",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ed52ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# script_dir = os.path.dirname(r\"C:\\Users\\ericdao\\Downloads\\\\\") #to specify another directory to output to\n",
    "\n",
    "script_dir = os.getcwd()\n",
    "out_folder = str(datetime.date.today()) + \"_output\"\n",
    "abs_path = os.path.join(script_dir, out_folder)\n",
    "\n",
    "if os.path.exists(abs_path):\n",
    "    shutil.rmtree(abs_path)\n",
    "os.mkdir(abs_path)\n",
    "\n",
    "\n",
    "# set up webscraping driver\n",
    "chromedriver = './chromedriver_100' #mac doesn't requite extenstion in name, whereas, windows does. Driver download page: https://chromedriver.chromium.org/downloads\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_experimental_option('excludeSwitches', ['enable-logging'])  #prevents logging of some unecessary driver related logs\n",
    "driver = webdriver.Chrome(chromedriver, options=options) \n",
    "\n",
    "# url = \"https://google.com\"\n",
    "# driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e83dd94-a2be-4920-af66-788c244385e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#grab the US song titles\n",
    "\n",
    "page = requests.get(\"https://www.billboard.com/charts/year-end/2021/hot-100-songs\")\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "job_elements = soup.find_all(\"h3\", class_=\"c-title\") \n",
    "song_titles = []\n",
    "for song_title in job_elements:\n",
    "    song_titles.append(song_title.text.strip())\n",
    "\n",
    "song_titles_100 = song_titles[:100] #verify that that this list contains all the song titles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "544613de-99bf-4edd-8a2f-61c0d4bc3519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error Drivers License Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"div[jsname=\"WbKHeb\"]\"}\n",
      "  (Session info: chrome=100.0.4896.127)\n",
      "\n",
      "error Beautiful Mistakes Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"div[jsname=\"WbKHeb\"]\"}\n",
      "  (Session info: chrome=100.0.4896.127)\n",
      "\n",
      "error Kings & Queens Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"div[jsname=\"WbKHeb\"]\"}\n",
      "  (Session info: chrome=100.0.4896.127)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use song titles to search the lyrics. Will need to manually fetch and input any song lyrics that weren't able to be automatically web scraped\n",
    "\n",
    "file_name = \"us_song_lyrics_2021\"\n",
    "lyrics = []\n",
    "with open((abs_path + \"//\" + file_name +'.csv'), 'w', newline='') as f:\n",
    "    write = csv.writer(f)\n",
    "    for song in song_titles_100:\n",
    "        try:\n",
    "            matched_elements = driver.get(\"https://www.google.com/search?q=\" + song + \" lyrics\")\n",
    "            time.sleep(1)\n",
    "            x = driver.find_element_by_css_selector('div[jsname=\"WbKHeb\"]')\n",
    "            lyrics.append(x.text)\n",
    "            write.writerow([x.text])\n",
    "        except Exception as e:\n",
    "            print(\"error | \",song, \" | \", e) # there will be some lyrics that won't automatically show up in google's first search result. These will need to be manually found and inserted into the us_song_lyrics csv file\n",
    "            pass\n",
    "    f.close()    \n",
    "    print(len(lyrics) + \" songs' lyrics retrieved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "acc98dc1-a2af-47b3-a054-97c7d2a6100d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# webscaper for tw song lyrics. Some song indexes will give an error like \"frame disconnected\", I think this relates to maybe an error casued by trying to scrape something when the page hasn't loaded completely yet maybe. I played around with putting wait intervals between different web scraping steps, and that seemed to help, but I need to spend more time to figure out exactly what causes that error in this script section\n",
    "\n",
    "driver = webdriver.Chrome(chromedriver, options=options) \n",
    "\n",
    "file_name = \"tw_song_lyrics_2020\"\n",
    "songs = []\n",
    "with open((abs_path + \"//\" + file_name +'.csv'), 'w', newline='', encoding=\"UTF-8\") as f:\n",
    "    write = csv.writer(f)\n",
    "    for i in range(0,202,2):\n",
    "        try:\n",
    "            driver.get(\"https://kma.kkbox.com/charts/yearly/newrelease?cate=297&lang=tc&terr=tw&year=2020#\")\n",
    "            time.sleep(5)\n",
    "            page = driver.find_elements_by_class_name(\"charts-list-cover\")\n",
    "            time.sleep(2)\n",
    "            new_page = page[i].click()\n",
    "            time.sleep(3)\n",
    "            page = requests.get(driver.current_url)\n",
    "            time.sleep(3)\n",
    "\n",
    "            soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "            time.sleep(2)\n",
    "            job_elements = soup.find(\"div\", class_=\"lyrics\")\n",
    "            time.sleep(3)\n",
    "            raw_text = job_elements.getText().splitlines()\n",
    "            while True:\n",
    "                for a in range(len(raw_text)): \n",
    "                    author_line_1 = raw_text[a].find('：') #-1 will be ruturned for find() if no match is found\n",
    "                    author_line_2 = raw_text[a].find(':')\n",
    "                    if (author_line_1 == -1 and author_line_2 == -1) and raw_text[a] != '': #this will skip those first few lines with the \"：\" symbol and also the blank lines\n",
    "                        break #once the first line is found with the ： and isn't blank, the loop will break and now a is the index of the start of the actual song to extract\n",
    "                break\n",
    "            text = raw_text[a:]\n",
    "            text = str(\"\\n\".join(text))\n",
    "            text = text.replace(\"\\n\", \" \")\n",
    "            songs.append(text)\n",
    "            write.writerow([text])\n",
    "            print(i)\n",
    "        except Exception as e:\n",
    "            print(\"error\",i, e)\n",
    "            pass\n",
    "    f.close()  \n",
    "    print(len(songs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "de482f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# manually fetch any tw song indexes that weren't sucessfully pulled and append to existing csv\n",
    "\n",
    "driver = webdriver.Chrome(chromedriver, options=options) \n",
    "\n",
    "file_name = \"tw_song_lyrics_2021\"\n",
    "songs = []\n",
    "with open((abs_path + \"//\" + file_name +'.csv'), 'a', newline='', encoding=\"UTF-8\") as f: #adjust directory path\n",
    "    write = csv.writer(f)\n",
    "    for i in ([22]):\n",
    "        try:\n",
    "            driver.get(\"https://kma.kkbox.com/charts/yearly/newrelease?cate=297&lang=tc&terr=tw&year=2021#\") #adjust year\n",
    "            time.sleep(5)\n",
    "            page = driver.find_elements_by_class_name(\"charts-list-cover\")\n",
    "            time.sleep(2)\n",
    "            new_page = page[i].click()\n",
    "            time.sleep(3)\n",
    "            page = requests.get(driver.current_url)\n",
    "            time.sleep(3)\n",
    "\n",
    "            soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "            time.sleep(2)\n",
    "            job_elements = soup.find(\"div\", class_=\"lyrics\")\n",
    "            time.sleep(3)\n",
    "            raw_text = job_elements.getText().splitlines()\n",
    "            while True:\n",
    "                for a in range(len(raw_text)): \n",
    "                    author_line_1 = raw_text[a].find('：') #-1 will be ruturned for find() if no match is found\n",
    "                    author_line_2 = raw_text[a].find(':')\n",
    "                    if (author_line_1 == -1 and author_line_2 == -1) and raw_text[a] != '': #this will skip those first few lines with the \"：\" symbol and also the blank lines\n",
    "                        break #once the first line is found with the ： and isn't blank, the loop will break and now a is the index of the start of the actual song to extract\n",
    "                break\n",
    "            text = raw_text[a:]\n",
    "            text = str(\"\\n\".join(text))\n",
    "            text = text.replace(\"\\n\", \" \")\n",
    "            songs.append(text)\n",
    "            write.writerow([text])\n",
    "            print(i)\n",
    "        except Exception as e:\n",
    "            print(\"error\",i, e)\n",
    "            pass\n",
    "    f.close()  \n",
    "    print(len(songs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert manually obtained lyrics into csv\n",
    "\n",
    "lyrics = \"\"\"我們走過了多少的傷害\n",
    "然後經歷了又一次失敗\n",
    "I know i’ll be alright\n",
    "Everything will be alright alright\n",
    "\n",
    "我們走過了多少的傷害\n",
    "然後經歷了又一次失敗\n",
    "I know i’ll be alright\n",
    "Everything will be alright alright be alright\n",
    "\n",
    "Ya 放輕鬆 全部重新來過\n",
    "繼續 向前走 把煩惱丟走\n",
    "不再留戀 別再回頭\n",
    "We moving on run n run like a circle\n",
    "認真的去感受頻率\n",
    "希望停止那些回憶\n",
    "回憶都只是過去 讓他過去 過去\n",
    "\n",
    "我們走過了多少的傷害（傷害）\n",
    "然後經歷了又一次失敗\n",
    "I know i’ll be alright（i know i’ll be alright）\n",
    "Everything will be alright alright\n",
    "\n",
    "我們走過了多少的傷害（傷害）\n",
    "然後經歷了又一次失敗\n",
    "I know i’ll be alright（i know i’ll be alright）\n",
    "Everything will be alright alright be alright\n",
    "Be alright i i’ll be alright\n",
    "Be alright i i’ll be alright（i know i’ll be alright）\n",
    "Be alright i i’ll be alright\n",
    "Be alright i i’ll be alright\n",
    "（everything will be alright alright be alright）\n",
    "\n",
    "獨自一人走在大街\n",
    "孤獨一人聽著音樂\n",
    "不敢再犯錯我一步一步的走\n",
    "曾經想要解脫 但又從頭來過\n",
    "不再回頭 保持初心向前走\n",
    "所有煩惱拋腦後\n",
    "認真的過每天生活\n",
    "\n",
    "I’ll go insane if i stay\n",
    "All this way with no change all day\n",
    "Undo the pain\n",
    "Freshen up\n",
    "Get back up\n",
    "Moving on go on\n",
    "\n",
    "我們走過了多少的傷害（傷害）\n",
    "然後經歷了又一次失敗\n",
    "I know i’ll be alright（i know i’ll be alright）\n",
    "Everything will be alright alright（be alright）\n",
    "我們走過了多少的傷害（傷害）\n",
    "然後經歷了又一次失敗\n",
    "I know i’ll be alright（i know i’ll be alright）\n",
    "Everything will be alright alright be alright\n",
    "\n",
    "Be alright i i’ll be alright\n",
    "Be alright i i’ll be alright（i know i’ll be alright）\n",
    "Be alright i i’ll be alright\n",
    "Be alright i i’ll be alright\n",
    "（everything will be alright alright be alright）\n",
    "\n",
    "Be alright i i’ll be alright\n",
    "Be alright i i’ll be alright（i know i’ll be alright）\n",
    "Be alright i i’ll be alright\n",
    "Be alright i i’ll be alright\n",
    "Everything will be alright alright be alright\"\"\"\n",
    "\n",
    "raw_text = lyrics.splitlines()\n",
    "\n",
    "with open((abs_path + \"//\" + file_name +'.csv'), 'a', newline='', encoding=\"UTF-8\") as f:\n",
    "    write = csv.writer(f)\n",
    "    while True:\n",
    "        for a in range(len(raw_text)): \n",
    "            author_line_1 = raw_text[a].find('：') #-1 will be ruturned for find() if no match is found\n",
    "            author_line_2 = raw_text[a].find(':')\n",
    "            if (author_line_1 == -1 and author_line_2 == -1) and raw_text[a] != '': #this will skip those first few lines with the \"：\" symbol and also the blank lines\n",
    "                break #once the first line is found with the ： and isn't blank, the loop will break and now a is the index of the start of the actual song to extract\n",
    "        break\n",
    "    text = raw_text[a:]\n",
    "    text = str(\"\\n\".join(text))\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    songs.append(text)\n",
    "    write.writerow([text])          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "898ffa83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output each individual word count for tw words\n",
    "\n",
    "file_name = \"tw_song_lyrics_2020\"\n",
    "\n",
    "\n",
    "# flattten song lyric list to prepare for segmentation\n",
    "df = pd.read_csv(abs_path + \"//\" + file_name + \".csv\", encoding=\"UTF-8\", header=None)\n",
    "songs_to_segment = df.values.tolist()\n",
    "flattened_song_list = ' '.join([data for song in songs_to_segment for data in song])\n",
    "\n",
    "\n",
    "# declare the segmentor. embedding=\"w2v\" will use the baseline model, \"elmo\" is supposely more accurate, but slower\n",
    "segmentor = seg.Wordseg(batch_size=64, device=\"cpu\", embedding='elmo', elmo_use_cuda=False, mode=\"TW\")\n",
    "\n",
    "# input is a list of string sentences that need to be flatenned.\n",
    "lyric_segmentation = segmentor.cut([flattened_song_list])\n",
    "# segmentor.cut([\"今天天氣真好啊! 潮水退了就知道，誰沒穿褲子。\"])\n",
    "\n",
    "\n",
    "# remove punctuation characters (will also remove any words that include a puncuation character)\n",
    "lyric_segmentation = [word for word in lyric_segmentation if word.isalpha()] \n",
    "\n",
    "# remove stop words\n",
    "stop_words = set(read_stopwords_list())\n",
    "lyric_rm_stopwords_segmentation = [w for w in lyric_segmentation if not w in stop_words]\n",
    "\n",
    "# calculate each word's word count\n",
    "def countOccurrence(a):\n",
    "  k = {}\n",
    "  for j in a:\n",
    "    if j in k:\n",
    "      k[j] +=1\n",
    "    else:\n",
    "      k[j] =1\n",
    "  return k\n",
    "lyric_segmentation_count = countOccurrence(lyric_segmentation[0])\n",
    "\n",
    "# sort word count\n",
    "sorted_keys = sorted(lyric_segmentation_count, key=lyric_segmentation_count.get, reverse=True)\n",
    "sorted_lyric_segmentation_count = {}\n",
    "for w in sorted_keys:\n",
    "    sorted_lyric_segmentation_count[w] = lyric_segmentation_count[w]\n",
    "\n",
    "# convert dictionary to dataframe\n",
    "sorted_lyric_segmentation_count_df = pd.DataFrame(list(sorted_lyric_segmentation_count.items()))\n",
    "sorted_lyric_segmentation_count_df = sorted_lyric_segmentation_count_df.rename(columns={0: \"word\", 1: \"count\"})\n",
    "sorted_lyric_segmentation_count_df.to_csv (abs_path + \"//\" + file_name + \"_word_count.csv\", index = False, header=True, encoding=\"UTF-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97b1d866",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'stopwords-zht.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/ericdao/Documents/local_code/webscraper/song_webscraper.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ericdao/Documents/local_code/webscraper/song_webscraper.ipynb#ch0000093?line=0'>1</a>\u001b[0m stop_words \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(read_stopwords_list())\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/web_scraper/lib/python3.9/site-packages/TCSP/__init__.py:5\u001b[0m, in \u001b[0;36mread_stopwords_list\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/site-packages/TCSP/__init__.py?line=1'>2</a>\u001b[0m stop_words \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/site-packages/TCSP/__init__.py?line=3'>4</a>\u001b[0m \u001b[39m# reading the custom stopwords text file\u001b[39;00m\n\u001b[0;32m----> <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/site-packages/TCSP/__init__.py?line=4'>5</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mstopwords-zht.txt\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m ) \u001b[39mas\u001b[39;00m f :\n\u001b[1;32m      <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/site-packages/TCSP/__init__.py?line=5'>6</a>\u001b[0m     lines \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mreadlines()\n\u001b[1;32m      <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/site-packages/TCSP/__init__.py?line=6'>7</a>\u001b[0m     \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m lines:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'stopwords-zht.txt'"
     ]
    }
   ],
   "source": [
    "stop_words = set(read_stopwords_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8776905a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527eeb3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eba5397",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bae3223",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528a4e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(read_stopwords_list())\n",
    "lyric_rm_stopwords_segmentation = [w for w in lyric_segmentation if not w in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5d113206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>的</td>\n",
       "      <td>1664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>我</td>\n",
       "      <td>1421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>你</td>\n",
       "      <td>1031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>不</td>\n",
       "      <td>607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>是</td>\n",
       "      <td>455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>久</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>地</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>怕</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>變成</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>兩</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    word  count\n",
       "0      的   1664\n",
       "1      我   1421\n",
       "2      你   1031\n",
       "3      不    607\n",
       "4      是    455\n",
       "..   ...    ...\n",
       "145    久     27\n",
       "146    地     26\n",
       "147    怕     26\n",
       "148   變成     26\n",
       "149    兩     26\n",
       "\n",
       "[150 rows x 2 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_lyric_segmentation_count_df[:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85171ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e039104",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0670d4c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "636887f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output each individual word count for us songs\n",
    "\n",
    "file_name = \"us_song_lyrics_2020\"\n",
    "\n",
    "# flattten song lyric list to prepare for segmentation\n",
    "df = pd.read_csv(abs_path + \"//\" + file_name + \".csv\", encoding=\"UTF-8\", header=None)\n",
    "songs_to_segment = df.values.tolist()\n",
    "flattened_song_list_1 = ' '.join([data.lower() for song in songs_to_segment for data in song])\n",
    "\n",
    "flattened_song_list_2 = str(\"\\n\".join([flattened_song_list_1]))\n",
    "flattened_song_list_2 = flattened_song_list_2.replace(\"\\n\", \" \")\n",
    "\n",
    "# seperate contractions into two seperate words\n",
    "flattened_song_list_3 = contractions.fix(flattened_song_list_2) \n",
    "\n",
    "# input is a list of string sentences that need to be flatenned.\n",
    "lyric_segmentation = nltk.word_tokenize(flattened_song_list_3)\n",
    "\n",
    "# remove punctuation characters (will also remove any words that include a puncuation character)\n",
    "lyric_segmentation = [word for word in lyric_segmentation if word.isalpha()] \n",
    "\n",
    "# remove stop words\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "lyric_rm_stopwords_segmentation = [w for w in lyric_segmentation if not w in stop_words]\n",
    "\n",
    "\n",
    "# segment words\n",
    "lyric_pos = nltk.pos_tag(lyric_segmentation)\n",
    "lyric_rm_stopwords_pos = nltk.pos_tag(lyric_rm_stopwords_segmentation)\n",
    "\n",
    "\n",
    "# make it easier to categorize words in post by categorizing based on part of speech. Update- it turns out nltk's part of speech categorizer isn't the most accurate. For example the number of times that love is referenced in the feelings category is much less than without part of speech categorization. More investigation is needed to determine if it's worth using.\n",
    "def feelings(pos_tuple):\n",
    "    word, tag = pos_tuple\n",
    "    if tag.startswith((\"JJ\", \"RB\", \"VB\")):\n",
    "        return True\n",
    "    # return True\n",
    "lyric_feelings_segmentation = [word for word, tag in filter(\n",
    "    feelings,\n",
    "    lyric_rm_stopwords_pos\n",
    ")]\n",
    "\n",
    "def pronouns(pos_tuple):\n",
    "    word, tag = pos_tuple\n",
    "    if tag.startswith((\"PRP\")):\n",
    "        return True\n",
    "    elif word == \"i\":\n",
    "        return True\n",
    "lyric_pronouns_segmentation = [word for word, tag in filter(\n",
    "    pronouns,\n",
    "    lyric_pos\n",
    ")]\n",
    "\n",
    "def nouns(pos_tuple):\n",
    "    word, tag = pos_tuple\n",
    "    if tag.startswith((\"NN\")):\n",
    "        return True\n",
    "lyric_nouns_segmentation = [word for word, tag in filter(\n",
    "    nouns,\n",
    "    lyric_rm_stopwords_pos\n",
    ")]\n",
    "\n",
    "\n",
    "# calculate each word's word count\n",
    "def countOccurrence(a):\n",
    "  k = {}\n",
    "  for j in a:\n",
    "    if j in k:\n",
    "      k[j] +=1\n",
    "    else:\n",
    "      k[j] =1\n",
    "  return k\n",
    "\n",
    "# lyric_segmentation_count = countOccurrence(lyric_segmentation)\n",
    "# lyric_feelings_segmentation_count = countOccurrence(lyric_feelings)\n",
    "# lyric_pronouns_segmentation_count = countOccurrence(lyric_feelings)\n",
    "# lyric_segmentation_count_rm_stopwords = countOccurrence(lyric_rm_stopwords_segmentation)\n",
    "\n",
    "# frequency distributions\n",
    "frequency_distribution_all = nltk.FreqDist(lyric_segmentation)\n",
    "frequency_distribution_feelings = nltk.FreqDist(lyric_feelings_segmentation)\n",
    "frequency_distribution_pronouns = nltk.FreqDist(lyric_pronouns_segmentation)\n",
    "frequency_distribution_nouns = nltk.FreqDist(lyric_nouns_segmentation)\n",
    "frequency_distribution_rm_stopwords = nltk.FreqDist(lyric_rm_stopwords_segmentation)\n",
    "\n",
    "# make into dataframes\n",
    "frequency_distribution_all_df = pd.DataFrame(list(frequency_distribution_all.most_common())) # the nltk.FreqDist object has built in functions, like taking the most_common will sort the frequency distribution \n",
    "frequency_distribution_all_df.columns = [\"word\", \"count\"]\n",
    "\n",
    "frequency_distribution_feelings_df = pd.DataFrame(list(frequency_distribution_feelings.most_common()))\n",
    "frequency_distribution_feelings_df.columns = [\"word\", \"count\"]\n",
    "\n",
    "frequency_distribution_pronouns_df = pd.DataFrame(list(frequency_distribution_pronouns.most_common()))\n",
    "frequency_distribution_pronouns_df.columns = [\"word\", \"count\"]\n",
    "\n",
    "frequency_distribution_nouns_df = pd.DataFrame(list(frequency_distribution_nouns.most_common()))\n",
    "frequency_distribution_nouns_df.columns = [\"word\", \"count\"]\n",
    "\n",
    "frequency_distribution_rm_stopwords_df = pd.DataFrame(list(frequency_distribution_rm_stopwords.most_common()))\n",
    "frequency_distribution_rm_stopwords_df.columns = [\"word\", \"count\"]\n",
    "\n",
    "# output each category into csv\n",
    "frequency_distribution_all_df.to_csv (abs_path + \"//\" + file_name + \"_all_word_count.csv\", index = False, header=True, encoding=\"UTF-8\")\n",
    "frequency_distribution_feelings_df.to_csv (abs_path + \"//\" + file_name + \"_feelings_word_count.csv\", index = False, header=True, encoding=\"UTF-8\")\n",
    "frequency_distribution_pronouns_df.to_csv (abs_path + \"//\" + file_name + \"_pronouns_word_count.csv\", index = False, header=True, encoding=\"UTF-8\")\n",
    "frequency_distribution_nouns_df.to_csv (abs_path + \"//\" + file_name + \"_nouns_word_count.csv\", index = False, header=True, encoding=\"UTF-8\")\n",
    "frequency_distribution_rm_stopwords_df.to_csv (abs_path + \"//\" + file_name + \"_rm_stopwords_word_count.csv\", index = False, header=True, encoding=\"UTF-8\")\n",
    "\n",
    "# compile into an excel file\n",
    "def excel_export():\n",
    "    global tab\n",
    "    tab = \"compiled\"\n",
    "    all_files = glob.glob(os.path.join(abs_path + \"//\", (file_name + \"*.csv\"))) # Find all files that have a .csv extenstion and certain prefix\n",
    "    all_files.sort(key=os.path.getctime)\n",
    "\n",
    "    writer = pd.ExcelWriter(os.path.join(abs_path + \"//\" + file_name + \"_\" + tab + \".xlsx\"), engine='xlsxwriter')\n",
    "\n",
    "    for f in all_files:\n",
    "        df = pd.read_csv(f)\n",
    "        df.to_excel(writer, sheet_name=os.path.splitext(os.path.basename(f))[0][20:], index=False)\n",
    "    writer.save()\n",
    "    \n",
    "excel_export()\n",
    "\n",
    "# Manual frequency distribution method\n",
    "# # sort word count\n",
    "# sorted_keys = sorted(lyric_segmentation_count, key=lyric_segmentation_count.get, reverse=True)\n",
    "# sorted_lyric_segmentation_count = {}\n",
    "# for w in sorted_keys:\n",
    "#     sorted_lyric_segmentation_count[w] = lyric_segmentation_count[w]\n",
    "\n",
    "# # sort word count\n",
    "# sorted_keys_rm_stopwords = sorted(lyric_segmentation_count_rm_stopwords, key=lyric_segmentation_count_rm_stopwords.get, reverse=True)\n",
    "# sorted_lyric_segmentation_rm_stopwords_count = {}\n",
    "# for w in sorted_keys_rm_stopwords:\n",
    "#     sorted_lyric_segmentation_rm_stopwords_count[w] = lyric_segmentation_count_rm_stopwords[w]\n",
    "\n",
    "# # convert dictionary to dataframe\n",
    "# sorted_lyric_segmentation_count_df = pd.DataFrame(list(sorted_lyric_segmentation_count.items()))\n",
    "# sorted_lyric_segmentation_count_df = sorted_lyric_segmentation_count_df.rename(columns={0: \"word\", 1: \"count\"})\n",
    "# frequency_distribution_all_df = pd.DataFrame(list(frequency_distribution_all.most_common()))\n",
    "# frequency_distribution_df = frequency_distribution_df.rename(columns={0: \"word\", 1: \"count\"})\n",
    "# sorted_lyric_segmentation_count_df.to_csv (abs_path + \"//\" + file_name + \"_word_count.csv\", index = False, header=True, encoding=\"UTF-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a7bb79b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for &: 'str' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/ericdao/Documents/local_code/webscraper/song_webscraper.ipynb Cell 14'\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ericdao/Documents/local_code/webscraper/song_webscraper.ipynb#ch0000052?line=5'>6</a>\u001b[0m pattern \u001b[39m=\u001b[39m \u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m[\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m string\u001b[39m.\u001b[39mpunctuation \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m、。〈〉《》\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ericdao/Documents/local_code/webscraper/song_webscraper.ipynb#ch0000052?line=6'>7</a>\u001b[0m \u001b[39m# Remove special characters from the string\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ericdao/Documents/local_code/webscraper/song_webscraper.ipynb#ch0000052?line=7'>8</a>\u001b[0m sample_str \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39;49mmatch(pattern, \u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m, sample_str)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ericdao/Documents/local_code/webscraper/song_webscraper.ipynb#ch0000052?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(sample_str)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/web_scraper/lib/python3.9/re.py:191\u001b[0m, in \u001b[0;36mmatch\u001b[0;34m(pattern, string, flags)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/re.py?line=187'>188</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmatch\u001b[39m(pattern, string, flags\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[1;32m    <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/re.py?line=188'>189</a>\u001b[0m     \u001b[39m\"\"\"Try to apply the pattern at the start of the string, returning\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/re.py?line=189'>190</a>\u001b[0m \u001b[39m    a Match object, or None if no match was found.\"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/re.py?line=190'>191</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m _compile(pattern, flags)\u001b[39m.\u001b[39mmatch(string)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/web_scraper/lib/python3.9/re.py:304\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(pattern, flags)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/re.py?line=301'>302</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m sre_compile\u001b[39m.\u001b[39misstring(pattern):\n\u001b[1;32m    <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/re.py?line=302'>303</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mfirst argument must be string or compiled pattern\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/re.py?line=303'>304</a>\u001b[0m p \u001b[39m=\u001b[39m sre_compile\u001b[39m.\u001b[39;49mcompile(pattern, flags)\n\u001b[1;32m    <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/re.py?line=304'>305</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (flags \u001b[39m&\u001b[39m DEBUG):\n\u001b[1;32m    <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/re.py?line=305'>306</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(_cache) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m _MAXCACHE:\n\u001b[1;32m    <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/re.py?line=306'>307</a>\u001b[0m         \u001b[39m# Drop the oldest item\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/web_scraper/lib/python3.9/sre_compile.py:764\u001b[0m, in \u001b[0;36mcompile\u001b[0;34m(p, flags)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/sre_compile.py?line=761'>762</a>\u001b[0m \u001b[39mif\u001b[39;00m isstring(p):\n\u001b[1;32m    <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/sre_compile.py?line=762'>763</a>\u001b[0m     pattern \u001b[39m=\u001b[39m p\n\u001b[0;32m--> <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/sre_compile.py?line=763'>764</a>\u001b[0m     p \u001b[39m=\u001b[39m sre_parse\u001b[39m.\u001b[39;49mparse(p, flags)\n\u001b[1;32m    <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/sre_compile.py?line=764'>765</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/sre_compile.py?line=765'>766</a>\u001b[0m     pattern \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/web_scraper/lib/python3.9/sre_parse.py:948\u001b[0m, in \u001b[0;36mparse\u001b[0;34m(str, flags, state)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/sre_parse.py?line=944'>945</a>\u001b[0m state\u001b[39m.\u001b[39mstr \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m\n\u001b[1;32m    <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/sre_parse.py?line=946'>947</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/sre_parse.py?line=947'>948</a>\u001b[0m     p \u001b[39m=\u001b[39m _parse_sub(source, state, flags \u001b[39m&\u001b[39;49m SRE_FLAG_VERBOSE, \u001b[39m0\u001b[39m)\n\u001b[1;32m    <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/sre_parse.py?line=948'>949</a>\u001b[0m \u001b[39mexcept\u001b[39;00m Verbose:\n\u001b[1;32m    <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/sre_parse.py?line=949'>950</a>\u001b[0m     \u001b[39m# the VERBOSE flag was switched on inside the pattern.  to be\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/sre_parse.py?line=950'>951</a>\u001b[0m     \u001b[39m# on the safe side, we'll parse the whole thing again...\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/sre_parse.py?line=951'>952</a>\u001b[0m     state \u001b[39m=\u001b[39m State()\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for &: 'str' and 'int'"
     ]
    }
   ],
   "source": [
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "sample_str = \"Test&[1]%{}$#$%-+String》\"\n",
    "# Create a regex pattern to match all special characters in string\n",
    "pattern = r'[' + string.punctuation + \"、。〈〉《》\" + ']'\n",
    "# Remove special characters from the string\n",
    "sample_str = re.sub(pattern, '', sample_str)\n",
    "print(sample_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "46949f67",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "multiple repeat at position 10",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m/Users/ericdao/Documents/local_code/webscraper/song_webscraper.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ericdao/Documents/local_code/webscraper/song_webscraper.ipynb#ch0000044?line=0'>1</a>\u001b[0m test \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39m!\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfs#\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mdfa?\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ericdao/Documents/local_code/webscraper/song_webscraper.ipynb#ch0000044?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m test:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ericdao/Documents/local_code/webscraper/song_webscraper.ipynb#ch0000044?line=4'>5</a>\u001b[0m     \u001b[39mif\u001b[39;00m re\u001b[39m.\u001b[39;49mmatch(\u001b[39m\"\"\"\u001b[39;49m\u001b[39m!\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m#$\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39m&\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m()*+,-./:;<=>?@[\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39m]^_`\u001b[39;49m\u001b[39m{\u001b[39;49m\u001b[39m|}~\u001b[39;49m\u001b[39m\"\"\"\u001b[39;49m, i):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ericdao/Documents/local_code/webscraper/song_webscraper.ipynb#ch0000044?line=5'>6</a>\u001b[0m         \u001b[39mprint\u001b[39m(i)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/web_scraper/lib/python3.9/re.py:191\u001b[0m, in \u001b[0;36mmatch\u001b[0;34m(pattern, string, flags)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/re.py?line=187'>188</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmatch\u001b[39m(pattern, string, flags\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[1;32m    <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/re.py?line=188'>189</a>\u001b[0m     \u001b[39m\"\"\"Try to apply the pattern at the start of the string, returning\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/re.py?line=189'>190</a>\u001b[0m \u001b[39m    a Match object, or None if no match was found.\"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/re.py?line=190'>191</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m _compile(pattern, flags)\u001b[39m.\u001b[39mmatch(string)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/web_scraper/lib/python3.9/re.py:304\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(pattern, flags)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/re.py?line=301'>302</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m sre_compile\u001b[39m.\u001b[39misstring(pattern):\n\u001b[1;32m    <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/re.py?line=302'>303</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mfirst argument must be string or compiled pattern\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/re.py?line=303'>304</a>\u001b[0m p \u001b[39m=\u001b[39m sre_compile\u001b[39m.\u001b[39;49mcompile(pattern, flags)\n\u001b[1;32m    <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/re.py?line=304'>305</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (flags \u001b[39m&\u001b[39m DEBUG):\n\u001b[1;32m    <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/re.py?line=305'>306</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(_cache) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m _MAXCACHE:\n\u001b[1;32m    <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/re.py?line=306'>307</a>\u001b[0m         \u001b[39m# Drop the oldest item\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/web_scraper/lib/python3.9/sre_compile.py:764\u001b[0m, in \u001b[0;36mcompile\u001b[0;34m(p, flags)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/sre_compile.py?line=761'>762</a>\u001b[0m \u001b[39mif\u001b[39;00m isstring(p):\n\u001b[1;32m    <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/sre_compile.py?line=762'>763</a>\u001b[0m     pattern \u001b[39m=\u001b[39m p\n\u001b[0;32m--> <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/sre_compile.py?line=763'>764</a>\u001b[0m     p \u001b[39m=\u001b[39m sre_parse\u001b[39m.\u001b[39;49mparse(p, flags)\n\u001b[1;32m    <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/sre_compile.py?line=764'>765</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/sre_compile.py?line=765'>766</a>\u001b[0m     pattern \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/web_scraper/lib/python3.9/sre_parse.py:948\u001b[0m, in \u001b[0;36mparse\u001b[0;34m(str, flags, state)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/sre_parse.py?line=944'>945</a>\u001b[0m state\u001b[39m.\u001b[39mstr \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m\n\u001b[1;32m    <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/sre_parse.py?line=946'>947</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/sre_parse.py?line=947'>948</a>\u001b[0m     p \u001b[39m=\u001b[39m _parse_sub(source, state, flags \u001b[39m&\u001b[39;49m SRE_FLAG_VERBOSE, \u001b[39m0\u001b[39;49m)\n\u001b[1;32m    <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/sre_parse.py?line=948'>949</a>\u001b[0m \u001b[39mexcept\u001b[39;00m Verbose:\n\u001b[1;32m    <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/sre_parse.py?line=949'>950</a>\u001b[0m     \u001b[39m# the VERBOSE flag was switched on inside the pattern.  to be\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/sre_parse.py?line=950'>951</a>\u001b[0m     \u001b[39m# on the safe side, we'll parse the whole thing again...\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/sre_parse.py?line=951'>952</a>\u001b[0m     state \u001b[39m=\u001b[39m State()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/web_scraper/lib/python3.9/sre_parse.py:443\u001b[0m, in \u001b[0;36m_parse_sub\u001b[0;34m(source, state, verbose, nested)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/sre_parse.py?line=440'>441</a>\u001b[0m start \u001b[39m=\u001b[39m source\u001b[39m.\u001b[39mtell()\n\u001b[1;32m    <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/sre_parse.py?line=441'>442</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/sre_parse.py?line=442'>443</a>\u001b[0m     itemsappend(_parse(source, state, verbose, nested \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m,\n\u001b[1;32m    <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/sre_parse.py?line=443'>444</a>\u001b[0m                        \u001b[39mnot\u001b[39;49;00m nested \u001b[39mand\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m items))\n\u001b[1;32m    <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/sre_parse.py?line=444'>445</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m sourcematch(\u001b[39m\"\u001b[39m\u001b[39m|\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/sre_parse.py?line=445'>446</a>\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/web_scraper/lib/python3.9/sre_parse.py:671\u001b[0m, in \u001b[0;36m_parse\u001b[0;34m(source, state, verbose, nested, first)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/sre_parse.py?line=667'>668</a>\u001b[0m     \u001b[39mraise\u001b[39;00m source\u001b[39m.\u001b[39merror(\u001b[39m\"\u001b[39m\u001b[39mnothing to repeat\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/sre_parse.py?line=668'>669</a>\u001b[0m                        source\u001b[39m.\u001b[39mtell() \u001b[39m-\u001b[39m here \u001b[39m+\u001b[39m \u001b[39mlen\u001b[39m(this))\n\u001b[1;32m    <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/sre_parse.py?line=669'>670</a>\u001b[0m \u001b[39mif\u001b[39;00m item[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m] \u001b[39min\u001b[39;00m _REPEATCODES:\n\u001b[0;32m--> <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/sre_parse.py?line=670'>671</a>\u001b[0m     \u001b[39mraise\u001b[39;00m source\u001b[39m.\u001b[39merror(\u001b[39m\"\u001b[39m\u001b[39mmultiple repeat\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/sre_parse.py?line=671'>672</a>\u001b[0m                        source\u001b[39m.\u001b[39mtell() \u001b[39m-\u001b[39m here \u001b[39m+\u001b[39m \u001b[39mlen\u001b[39m(this))\n\u001b[1;32m    <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/sre_parse.py?line=672'>673</a>\u001b[0m \u001b[39mif\u001b[39;00m item[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m] \u001b[39mis\u001b[39;00m SUBPATTERN:\n\u001b[1;32m    <a href='file:///Users/ericdao/opt/anaconda3/envs/web_scraper/lib/python3.9/sre_parse.py?line=673'>674</a>\u001b[0m     group, add_flags, del_flags, p \u001b[39m=\u001b[39m item[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31merror\u001b[0m: multiple repeat at position 10"
     ]
    }
   ],
   "source": [
    "test = [\"!\", \"fs#\", \"dfa?\"]\n",
    "\n",
    "\n",
    "for i in test:\n",
    "    if re.match(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\", i):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "c09218e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('song_us_cut.json',)\n",
    " \n",
    "data = json.load(f)\n",
    "\n",
    "data1 = sorted_lyric_segmentation_count\n",
    "\n",
    "for i in data1.copy().keys():\n",
    "    if re.match('\\W|.*\\'\\.*|\\d', i):\n",
    "        del data1[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "850cb068-e1be-4920-9911-811b10229114",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'get': 569,\n",
       " 'yeah': 532,\n",
       " 'like': 374,\n",
       " 'know': 326,\n",
       " 'go': 287,\n",
       " 'love': 256,\n",
       " 'oh': 231,\n",
       " 'say': 229,\n",
       " 'na': 224,\n",
       " 'ai': 204,\n",
       " 'baby': 197,\n",
       " 'wan': 195,\n",
       " 'one': 176,\n",
       " 'make': 174,\n",
       " 'come': 151,\n",
       " 'nigga': 149,\n",
       " 'want': 143,\n",
       " 'bitch': 140,\n",
       " 'let': 136,\n",
       " 'ca': 131,\n",
       " 'need': 129,\n",
       " 'night': 128,\n",
       " 'fuck': 126,\n",
       " 'tell': 121,\n",
       " 'see': 118,\n",
       " 'gon': 114,\n",
       " 'could': 108,\n",
       " 'never': 107,\n",
       " 'take': 106,\n",
       " 'back': 105,\n",
       " 'way': 104,\n",
       " 'time': 101,\n",
       " 'keep': 100,\n",
       " 'feel': 97,\n",
       " 'shit': 97,\n",
       " 'good': 96,\n",
       " 'ooh': 95,\n",
       " 'right': 95,\n",
       " 'woo': 94,\n",
       " 'heart': 85,\n",
       " 'girl': 85,\n",
       " 'put': 82,\n",
       " 'ayy': 80,\n",
       " 'think': 78,\n",
       " 'run': 73,\n",
       " 'still': 68,\n",
       " 'uh': 68,\n",
       " 'even': 67,\n",
       " 'big': 67,\n",
       " 'walk': 66,\n",
       " 'dance': 65,\n",
       " 'doo': 63,\n",
       " 'day': 62,\n",
       " 'hot': 62,\n",
       " 'ta': 61,\n",
       " 'call': 60,\n",
       " 'bad': 60,\n",
       " 'might': 60,\n",
       " 'rain': 60,\n",
       " 'thing': 59,\n",
       " 'turn': 57,\n",
       " 'leave': 57,\n",
       " 'wo': 56,\n",
       " 'got': 55,\n",
       " 'would': 55,\n",
       " 'hope': 55,\n",
       " 'try': 53,\n",
       " 'move': 51,\n",
       " 'lil': 51,\n",
       " 'bring': 51,\n",
       " 'give': 50,\n",
       " 'u': 50,\n",
       " 'stay': 50,\n",
       " 'ever': 49,\n",
       " 'play': 49,\n",
       " 'every': 48,\n",
       " 'end': 48,\n",
       " 'look': 46,\n",
       " 'well': 46,\n",
       " 'ah': 46,\n",
       " 'babe': 46,\n",
       " 'new': 45,\n",
       " 'tryna': 45,\n",
       " 'nobody': 44,\n",
       " 'fall': 44,\n",
       " 'around': 43,\n",
       " 'always': 43,\n",
       " 'man': 43,\n",
       " 'high': 43,\n",
       " 'la': 43,\n",
       " 'show': 42,\n",
       " 'away': 42,\n",
       " 'real': 42,\n",
       " 'two': 41,\n",
       " 'pussy': 41,\n",
       " 'really': 40,\n",
       " 'start': 40,\n",
       " 'life': 40,\n",
       " 'savage': 40,\n",
       " 'lose': 39,\n",
       " 'hit': 38,\n",
       " 'guy': 38,\n",
       " 'whole': 38,\n",
       " 'hear': 37,\n",
       " 'money': 37,\n",
       " 'light': 36,\n",
       " 'hand': 36,\n",
       " 'eye': 36,\n",
       " 'friend': 36,\n",
       " 'rhythm': 36,\n",
       " 'feeling': 35,\n",
       " 'watch': 35,\n",
       " 'ya': 35,\n",
       " 'hell': 35,\n",
       " 'damn': 34,\n",
       " 'fuckin': 34,\n",
       " 'goin': 34,\n",
       " 'fire': 34,\n",
       " 'mean': 34,\n",
       " 'break': 34,\n",
       " 'hey': 33,\n",
       " 'house': 33,\n",
       " 'stop': 33,\n",
       " 'mama': 33,\n",
       " 'slide': 33,\n",
       " 'everything': 33,\n",
       " 'long': 32,\n",
       " 'boy': 32,\n",
       " 'sometimes': 32,\n",
       " 'hard': 32,\n",
       " 'huh': 31,\n",
       " 'type': 31,\n",
       " 'world': 31,\n",
       " 'mind': 30,\n",
       " 'soul': 30,\n",
       " 'pull': 30,\n",
       " 'three': 30,\n",
       " 'hate': 30,\n",
       " 'woah': 30,\n",
       " 'party': 29,\n",
       " 'fly': 28,\n",
       " 'last': 28,\n",
       " 'head': 28,\n",
       " 'sugar': 28,\n",
       " 'much': 27,\n",
       " 'cold': 27,\n",
       " 'niggas': 27,\n",
       " 'lonely': 27,\n",
       " 'wait': 26,\n",
       " 'stick': 26,\n",
       " 'crazy': 26,\n",
       " 'okay': 26,\n",
       " 'little': 26,\n",
       " 'nah': 26,\n",
       " 'young': 25,\n",
       " 'wish': 25,\n",
       " 'spend': 25,\n",
       " 'top': 25,\n",
       " 'ay': 25,\n",
       " 'ask': 25,\n",
       " 'watermelon': 25,\n",
       " 'hundred': 24,\n",
       " 'care': 24,\n",
       " 'guess': 24,\n",
       " 'without': 24,\n",
       " 'dream': 24,\n",
       " 'throw': 24,\n",
       " 'find': 24,\n",
       " 'pay': 24,\n",
       " 'phone': 23,\n",
       " 'change': 23,\n",
       " 'lot': 23,\n",
       " 'drop': 23,\n",
       " 'talk': 23,\n",
       " 'street': 23,\n",
       " 'game': 23,\n",
       " 'miss': 23,\n",
       " 'foot': 23,\n",
       " 'room': 23,\n",
       " 'something': 22,\n",
       " 'buy': 22,\n",
       " 'band': 22,\n",
       " 'somebody': 22,\n",
       " 'hurt': 22,\n",
       " 'bed': 22,\n",
       " 'tonight': 22,\n",
       " 'though': 21,\n",
       " 'lay': 21,\n",
       " 'shawty': 21,\n",
       " 'dick': 21,\n",
       " 'car': 21,\n",
       " 'kill': 21,\n",
       " 'talkin': 21,\n",
       " 'nothing': 21,\n",
       " 'die': 21,\n",
       " 'face': 21,\n",
       " 'name': 21,\n",
       " 'beat': 21,\n",
       " 'alone': 21,\n",
       " 'happen': 21,\n",
       " 'best': 21,\n",
       " 'breathe': 21,\n",
       " 'bag': 20,\n",
       " 'ice': 20,\n",
       " 'doin': 20,\n",
       " 'someone': 20,\n",
       " 'trap': 20,\n",
       " 'first': 20,\n",
       " 'whoa': 20,\n",
       " 'straight': 20,\n",
       " 'wet': 20,\n",
       " 'road': 19,\n",
       " 'believe': 19,\n",
       " 'brand': 19,\n",
       " 'hold': 19,\n",
       " 'check': 19,\n",
       " 'shoot': 19,\n",
       " 'diamond': 19,\n",
       " 'rich': 19,\n",
       " 'use': 19,\n",
       " 'close': 19,\n",
       " 'bae': 19,\n",
       " 'hoe': 19,\n",
       " 'gettin': 19,\n",
       " 'home': 19,\n",
       " 'n': 19,\n",
       " 'city': 18,\n",
       " 'whip': 18,\n",
       " 'drug': 18,\n",
       " 'god': 18,\n",
       " 'lie': 18,\n",
       " 'work': 18,\n",
       " 'lock': 18,\n",
       " 'cry': 18,\n",
       " 'hair': 18,\n",
       " 'feelin': 18,\n",
       " 'extremo': 18,\n",
       " 'margarita': 18,\n",
       " 'word': 17,\n",
       " 'ride': 17,\n",
       " 'thousand': 17,\n",
       " 'deep': 17,\n",
       " 'stand': 17,\n",
       " 'memory': 17,\n",
       " 'kinda': 17,\n",
       " 'sing': 17,\n",
       " 'free': 17,\n",
       " 'wrong': 17,\n",
       " 'touch': 16,\n",
       " 'sky': 16,\n",
       " 'pop': 16,\n",
       " 'side': 16,\n",
       " 'block': 16,\n",
       " 'drink': 16,\n",
       " 'drive': 16,\n",
       " 'set': 16,\n",
       " 'song': 16,\n",
       " 'learn': 16,\n",
       " 'else': 16,\n",
       " 'yo': 16,\n",
       " 'old': 16,\n",
       " 'toss': 16,\n",
       " 'fine': 16,\n",
       " 'na-na': 16,\n",
       " 'shorty': 16,\n",
       " 'yummy-yum': 16,\n",
       " 'christmas': 16,\n",
       " 'sum': 16,\n",
       " 'la-la-la-la': 16,\n",
       " 'maybe': 15,\n",
       " 'trust': 15,\n",
       " 'cash': 15,\n",
       " 'nothin': 15,\n",
       " 'meet': 15,\n",
       " 'glock': 15,\n",
       " 'hop': 15,\n",
       " 'pain': 15,\n",
       " 'front': 15,\n",
       " 'inside': 15,\n",
       " 'honey': 15,\n",
       " 'glass': 15,\n",
       " 'everybody': 15,\n",
       " 'help': 15,\n",
       " 'roll': 15,\n",
       " 'listen': 15,\n",
       " 'whore': 15,\n",
       " 'fight': 15,\n",
       " 'rack': 15,\n",
       " 'gang': 15,\n",
       " 'heartless': 15,\n",
       " 'chase': 15,\n",
       " 'cup': 15,\n",
       " 'la-la-la': 15,\n",
       " 'roxanne': 15,\n",
       " 'mouth': 14,\n",
       " 'already': 14,\n",
       " 'wake': 14,\n",
       " 'daddy': 14,\n",
       " 'smoke': 14,\n",
       " 'swear': 14,\n",
       " 'please': 14,\n",
       " 'ring': 14,\n",
       " 'plan': 14,\n",
       " 'rock': 14,\n",
       " 'ten': 14,\n",
       " 'people': 14,\n",
       " 'stupid': 14,\n",
       " 'ap': 14,\n",
       " 'drip': 14,\n",
       " 'perfect': 14,\n",
       " 'jean': 14,\n",
       " 'cut': 14,\n",
       " 'shot': 14,\n",
       " 'sorry': 14,\n",
       " 'molly': 14,\n",
       " 'monster': 14,\n",
       " 'enough': 13,\n",
       " 'sleep': 13,\n",
       " 'wear': 13,\n",
       " 'goodbye': 13,\n",
       " 'soon': 13,\n",
       " 'fill': 13,\n",
       " 'many': 13,\n",
       " 'forever': 13,\n",
       " 'felt': 13,\n",
       " 'along': 13,\n",
       " 'kiss': 13,\n",
       " 'sayin': 13,\n",
       " 'black': 13,\n",
       " 'past': 13,\n",
       " 'nail': 13,\n",
       " 'door': 13,\n",
       " 'alive': 13,\n",
       " 'dynamite': 13,\n",
       " 'least': 13,\n",
       " 'definition': 13,\n",
       " 'water': 12,\n",
       " 'full': 12,\n",
       " 'cop': 12,\n",
       " 'another': 12,\n",
       " 'catch': 12,\n",
       " 'bottom': 12,\n",
       " 'since': 12,\n",
       " 'dry': 12,\n",
       " 'happy': 12,\n",
       " 'forget': 12,\n",
       " 'ball': 12,\n",
       " 'worry': 12,\n",
       " 'great': 12,\n",
       " 'lovin': 12,\n",
       " 'pray': 12,\n",
       " 'live': 12,\n",
       " 'star': 12,\n",
       " 'chasing': 12,\n",
       " 'five': 12,\n",
       " 'white': 12,\n",
       " 'tú': 12,\n",
       " 'stuck': 12,\n",
       " 'scar': 12,\n",
       " 'outside': 12,\n",
       " 'suicidal': 12,\n",
       " 'rags': 12,\n",
       " 'ruin': 12,\n",
       " 'pullin': 11,\n",
       " 'told': 11,\n",
       " 'pour': 11,\n",
       " 'hood': 11,\n",
       " 'mine': 11,\n",
       " 'adore': 11,\n",
       " 'crib': 11,\n",
       " 'number': 11,\n",
       " 'news': 11,\n",
       " 'remember': 11,\n",
       " 'alright': 11,\n",
       " 'left': 11,\n",
       " 'rest': 11,\n",
       " 'fear': 11,\n",
       " 'easy': 11,\n",
       " 'morning': 11,\n",
       " 'fast': 11,\n",
       " 'middle': 11,\n",
       " 'wild': 11,\n",
       " 'body': 11,\n",
       " 'taste': 11,\n",
       " 'none': 11,\n",
       " 'mm': 11,\n",
       " 'year': 11,\n",
       " 'far': 11,\n",
       " 'next': 11,\n",
       " 'truth': 11,\n",
       " 'afraid': 11,\n",
       " 'town': 11,\n",
       " 'post': 11,\n",
       " 'que': 11,\n",
       " 'round': 11,\n",
       " 'cartier': 11,\n",
       " 'sad': 11,\n",
       " 'blind': 10,\n",
       " 'box': 10,\n",
       " 'brother': 10,\n",
       " 'wonder': 10,\n",
       " 'summer': 10,\n",
       " 'dark': 10,\n",
       " 'mess': 10,\n",
       " 'sound': 10,\n",
       " 'quick': 10,\n",
       " 'freak': 10,\n",
       " 'club': 10,\n",
       " 'somewhere': 10,\n",
       " 'sit': 10,\n",
       " 'piece': 10,\n",
       " 'lit': 10,\n",
       " 'picture': 10,\n",
       " 'killer': 10,\n",
       " 'coffee': 10,\n",
       " 'broke': 10,\n",
       " 'burn': 10,\n",
       " 'million': 10,\n",
       " 'fallin': 10,\n",
       " 'whatever': 10,\n",
       " 'lookin': 10,\n",
       " 'dy-na-na-na': 10,\n",
       " 'half': 10,\n",
       " 'problem': 10,\n",
       " 'true': 10,\n",
       " 'runnin': 10,\n",
       " 'mood': 10,\n",
       " 'living': 10,\n",
       " 'four': 10,\n",
       " 'riches': 10,\n",
       " 'pipe': 10,\n",
       " 'sun': 9,\n",
       " 'coupe': 9,\n",
       " 'probably': 9,\n",
       " 'key': 9,\n",
       " 'wood': 9,\n",
       " 'safe': 9,\n",
       " 'air': 9,\n",
       " 'boom': 9,\n",
       " 'somethin': 9,\n",
       " 'pass': 9,\n",
       " 'turnt': 9,\n",
       " 'dumb': 9,\n",
       " 'red': 9,\n",
       " 'anybody': 9,\n",
       " 'sure': 9,\n",
       " 'matter': 9,\n",
       " 'storm': 9,\n",
       " 'bleed': 9,\n",
       " 'grab': 9,\n",
       " 'actin': 9,\n",
       " 'mad': 9,\n",
       " 'pretty': 9,\n",
       " 'dead': 9,\n",
       " 'sweet': 9,\n",
       " 'attention': 9,\n",
       " 'warm': 9,\n",
       " 'everyone': 9,\n",
       " 'kid': 9,\n",
       " 'either': 9,\n",
       " 'kick': 9,\n",
       " 'bottle': 9,\n",
       " 'view': 9,\n",
       " 'reason': 9,\n",
       " 'benz': 9,\n",
       " 'slow': 9,\n",
       " 'givin': 9,\n",
       " 'awake': 9,\n",
       " 'outta': 9,\n",
       " 'part': 9,\n",
       " 'dior': 9,\n",
       " 'lo': 9,\n",
       " 'bom': 9,\n",
       " 'count': 9,\n",
       " 'bitches': 9,\n",
       " 'hennessy': 9,\n",
       " 'line': 9,\n",
       " 'juicy': 9,\n",
       " 'tantrum': 9,\n",
       " 'bummer': 9,\n",
       " 'circle': 8,\n",
       " 'sex': 8,\n",
       " 'skrrt': 8,\n",
       " 'playin': 8,\n",
       " 'double': 8,\n",
       " 'deal': 8,\n",
       " 'chance': 8,\n",
       " 'fool': 8,\n",
       " 'gas': 8,\n",
       " 'choose': 8,\n",
       " 'wrist': 8,\n",
       " 'said': 8,\n",
       " 'cool': 8,\n",
       " 'fun': 8,\n",
       " 'save': 8,\n",
       " 'moment': 8,\n",
       " 'notice': 8,\n",
       " 'dress': 8,\n",
       " 'shy': 8,\n",
       " 'smile': 8,\n",
       " 'dime': 8,\n",
       " 'team': 8,\n",
       " 'trip': 8,\n",
       " 'intention': 8,\n",
       " 'mention': 8,\n",
       " 'act': 8,\n",
       " 'oh-oh': 8,\n",
       " 'hour': 8,\n",
       " 'jump': 8,\n",
       " 'chain': 8,\n",
       " 'country': 8,\n",
       " 'uh-huh': 8,\n",
       " 'love-love-love': 8,\n",
       " 'alight': 8,\n",
       " 'na-na-na': 8,\n",
       " 'clap': 8,\n",
       " 'player': 8,\n",
       " 'hater': 8,\n",
       " 'dope': 8,\n",
       " 'señorita': 8,\n",
       " 'sippin': 8,\n",
       " 'rather': 8,\n",
       " 'thief': 8,\n",
       " 'ridin': 8,\n",
       " 'gun': 8,\n",
       " 'fault': 8,\n",
       " 'sleeve': 8,\n",
       " 'human': 8,\n",
       " 'aye': 8,\n",
       " 'drinkin': 8,\n",
       " 'bro': 8,\n",
       " 'twenty': 8,\n",
       " 'yummy-yummy': 8,\n",
       " 'late': 8,\n",
       " 'stress': 8,\n",
       " 'wave': 8,\n",
       " 'popstar': 8,\n",
       " 'vvs': 8,\n",
       " 'unless': 8,\n",
       " 'prove': 8,\n",
       " 'mike': 8,\n",
       " 'amiri': 8,\n",
       " 'billie': 8,\n",
       " 'spotlight': 8,\n",
       " 'anthem': 8,\n",
       " 'malibu': 8,\n",
       " 'proud': 7,\n",
       " 'blame': 7,\n",
       " 'son': 7,\n",
       " 'draco': 7,\n",
       " 'blue': 7,\n",
       " 'kitchen': 7,\n",
       " 'pistol': 7,\n",
       " 'promise': 7,\n",
       " 'tear': 7,\n",
       " 'saw': 7,\n",
       " 'different': 7,\n",
       " 'cross': 7,\n",
       " 'nut': 7,\n",
       " 'twice': 7,\n",
       " 'finger': 7,\n",
       " 'dollar': 7,\n",
       " 'bentley': 7,\n",
       " 'birkin': 7,\n",
       " 'carry': 7,\n",
       " 'bone': 7,\n",
       " 'peel': 7,\n",
       " 'blow': 7,\n",
       " 'cheat': 7,\n",
       " 'rid': 7,\n",
       " 'hometown': 7,\n",
       " 'fan': 7,\n",
       " 'shine': 7,\n",
       " 'nasty': 7,\n",
       " 'smack': 7,\n",
       " 'thought': 7,\n",
       " 'whenever': 7,\n",
       " 'couple': 7,\n",
       " 'figure': 7,\n",
       " 'mil': 7,\n",
       " 'tire': 7,\n",
       " 'whiskey': 7,\n",
       " 'midnight': 7,\n",
       " 'point': 7,\n",
       " 'rubber': 7,\n",
       " 'laugh': 7,\n",
       " 'store': 7,\n",
       " 'rule': 7,\n",
       " 'bein': 7,\n",
       " 'otro': 7,\n",
       " 'fuego': 7,\n",
       " 'en': 7,\n",
       " 'nonstop': 7,\n",
       " 'clear': 7,\n",
       " 'glad': 7,\n",
       " 'bank': 7,\n",
       " 'rap': 7,\n",
       " 'bandit': 7,\n",
       " 'plane': 7,\n",
       " 'doctor': 7,\n",
       " 'quite': 7,\n",
       " 'map': 7,\n",
       " 'foreign': 7,\n",
       " 'empty': 6,\n",
       " 'feed': 6,\n",
       " 'bustin': 6,\n",
       " 'nose': 6,\n",
       " 'slatt': 6,\n",
       " 'sell': 6,\n",
       " 'patek': 6,\n",
       " 'took': 6,\n",
       " 'pick': 6,\n",
       " 'guitar': 6,\n",
       " 'squeeze': 6,\n",
       " 'ready': 6,\n",
       " 'dog': 6,\n",
       " 'wakin': 6,\n",
       " 'text': 6,\n",
       " 'send': 6,\n",
       " 'toe': 6,\n",
       " 'blood': 6,\n",
       " 'toast': 6,\n",
       " 'today': 6,\n",
       " 'paint': 6,\n",
       " 'shatter': 6,\n",
       " 'punch': 6,\n",
       " 'focused': 6,\n",
       " 'west': 6,\n",
       " 'hang': 6,\n",
       " 'section': 6,\n",
       " 'style': 6,\n",
       " 'floor': 6,\n",
       " 'spot': 6,\n",
       " 'gorgeous': 6,\n",
       " 'makin': 6,\n",
       " 'kind': 6,\n",
       " 'business': 6,\n",
       " 'raw': 6,\n",
       " 'lord': 6,\n",
       " 'drunk': 6,\n",
       " 'swing': 6,\n",
       " 'sign': 6,\n",
       " 'underneath': 6,\n",
       " 'brain': 6,\n",
       " 'story': 6,\n",
       " 'speak': 6,\n",
       " 'hah': 6,\n",
       " 'finish': 6,\n",
       " 'hearin': 6,\n",
       " 'anymore': 6,\n",
       " 'nike': 6,\n",
       " 'jackson': 6,\n",
       " 'thug': 6,\n",
       " 'hello': 6,\n",
       " 'tv': 6,\n",
       " 'bass': 6,\n",
       " 'shining': 6,\n",
       " 'funk': 6,\n",
       " 'minute': 6,\n",
       " 'watchin': 6,\n",
       " 'low': 6,\n",
       " 'toda': 6,\n",
       " 'noche': 6,\n",
       " 'rompemo': 6,\n",
       " 'al': 6,\n",
       " 'día': 6,\n",
       " 'volvemo': 6,\n",
       " 'sabes': 6,\n",
       " 'cómo': 6,\n",
       " 'hacemo': 6,\n",
       " 'dinero': 6,\n",
       " 'havin': 6,\n",
       " 'loved': 6,\n",
       " 'bi': 6,\n",
       " 'dum': 6,\n",
       " 'fourteen': 6,\n",
       " 'gram': 6,\n",
       " 'control': 6,\n",
       " 'sunday': 6,\n",
       " 'stole': 6,\n",
       " 'boo': 6,\n",
       " 'broken': 6,\n",
       " 'callin': 6,\n",
       " 'helicopter': 6,\n",
       " 'cops': 6,\n",
       " 'death': 6,\n",
       " 'six': 6,\n",
       " 'salt': 6,\n",
       " 'bullet': 6,\n",
       " 'attitude': 6,\n",
       " 'small': 6,\n",
       " 'nowhere': 6,\n",
       " 'de': 6,\n",
       " 'bluebird': 6,\n",
       " 'spring': 6,\n",
       " 'brr': 6,\n",
       " 'blazin': 6,\n",
       " 'bands': 6,\n",
       " 'flame': 5,\n",
       " 'lick': 5,\n",
       " 'mmh': 5,\n",
       " 'movin': 5,\n",
       " 'shoe': 5,\n",
       " 'scary': 5,\n",
       " 'anything': 5,\n",
       " 'chop': 5,\n",
       " 'bucket': 5,\n",
       " 'mop': 5,\n",
       " 'ballin': 5,\n",
       " 'opps': 5,\n",
       " 'spin': 5,\n",
       " 'weekend': 5,\n",
       " 'usual': 5,\n",
       " 'wine': 5,\n",
       " 'cheap': 5,\n",
       " 'truck': 5,\n",
       " 'fact': 5,\n",
       " 'crack': 5,\n",
       " 'luck': 5,\n",
       " 'heal': 5,\n",
       " 'guard': 5,\n",
       " 'rug': 5,\n",
       " 'must': 5,\n",
       " 'scream': 5,\n",
       " 'woman': 5,\n",
       " 'mile': 5,\n",
       " 'wreck': 5,\n",
       " 'poppin': 5,\n",
       " 'switch': 5,\n",
       " 'fit': 5,\n",
       " 'classy': 5,\n",
       " 'ratchet': 5,\n",
       " 'filter': 5,\n",
       " 'cookin': 5,\n",
       " 'bread': 5,\n",
       " 'asset': 5,\n",
       " 'tomorrow': 5,\n",
       " 'roses': 5,\n",
       " 'corner': 5,\n",
       " 'went': 5,\n",
       " 'week': 5,\n",
       " 'boot': 5,\n",
       " 'hook': 5,\n",
       " 'bust': 5,\n",
       " 'bop': 5,\n",
       " 'bit': 5,\n",
       " 'biggie': 5,\n",
       " 'drivin': 5,\n",
       " 'forest': 5,\n",
       " 'softly': 5,\n",
       " 'thick': 5,\n",
       " 'basically': 5,\n",
       " 'waste': 5,\n",
       " 'i-i-i': 5,\n",
       " 'stone': 5,\n",
       " 'everythin': 5,\n",
       " 'applause': 5,\n",
       " 'goyard': 5,\n",
       " 'mornin': 5,\n",
       " 'case': 5,\n",
       " 'land': 5,\n",
       " 'hotel': 5,\n",
       " 'dawg': 5,\n",
       " 'vibe': 5,\n",
       " 'livin': 5,\n",
       " 'v': 5,\n",
       " 'seat': 5,\n",
       " 'mi': 5,\n",
       " 'men': 5,\n",
       " 'chair': 5,\n",
       " 'finally': 5,\n",
       " 'hoppin': 5,\n",
       " 'sunset': 5,\n",
       " 'yum': 5,\n",
       " 'bless': 5,\n",
       " 'ear': 5,\n",
       " 'godzilla': 5,\n",
       " 'better': 5,\n",
       " 'walkin': 5,\n",
       " 'ease': 5,\n",
       " 'christian': 5,\n",
       " 'music': 5,\n",
       " 'leg': 5,\n",
       " 'seem': 5,\n",
       " 'breakin': 5,\n",
       " 'surprise': 5,\n",
       " 'shake': 5,\n",
       " 'extrañarte': 5,\n",
       " 'homesick': 5,\n",
       " 'wide': 5,\n",
       " 'hill': 5,\n",
       " 'average': 5,\n",
       " 'college': 5,\n",
       " 'poured': 5,\n",
       " 'blueberry': 5,\n",
       " 'faygo': 5,\n",
       " 'false': 5,\n",
       " 'thumbin': 5,\n",
       " 'judge': 4,\n",
       " 'bell': 4,\n",
       " 'seal': 4,\n",
       " 'lazy': 4,\n",
       " 'suck': 4,\n",
       " 'wipe': 4,\n",
       " 'daughter': 4,\n",
       " 'arm': 4,\n",
       " 'slip': 4,\n",
       " 'traffic': 4,\n",
       " 'gangsta': 4,\n",
       " 'earn': 4,\n",
       " 'push': 4,\n",
       " 'guarantee': 4,\n",
       " 'age': 4,\n",
       " 'wash': 4,\n",
       " 'lemon': 4,\n",
       " 'hunnid': 4,\n",
       " 'serve': 4,\n",
       " 'cocaine': 4,\n",
       " 'pill': 4,\n",
       " 'neck': 4,\n",
       " 'shirt': 4,\n",
       " 'worth': 4,\n",
       " 'reach': 4,\n",
       " 'memories': 4,\n",
       " 'nightfall': 4,\n",
       " 'fell': 4,\n",
       " 'read': 4,\n",
       " 'prolly': 4,\n",
       " 'knock': 4,\n",
       " 'spark': 4,\n",
       " 'goddamn': 4,\n",
       " 'yes': 4,\n",
       " 'blessing': 4,\n",
       " 'sick': 4,\n",
       " 'passion': 4,\n",
       " 'pack': 4,\n",
       " 'dig': 4,\n",
       " 'tune': 4,\n",
       " 'beg': 4,\n",
       " 'trash': 4,\n",
       " 'bougie': 4,\n",
       " 'sassy': 4,\n",
       " 'moody': 4,\n",
       " 'acting': 4,\n",
       " 'eat': 4,\n",
       " 'knot': 4,\n",
       " 'match': 4,\n",
       " 'shower': 4,\n",
       " 'equity': 4,\n",
       " 'create': 4,\n",
       " 'everywhere': 4,\n",
       " 'second': 4,\n",
       " 'cap': 4,\n",
       " 'honest': 4,\n",
       " 'deserve': 4,\n",
       " 'photo': 4,\n",
       " 'handle': 4,\n",
       " 'table': 4,\n",
       " 'loud': 4,\n",
       " 'weather': 4,\n",
       " 'wall': 4,\n",
       " 'favorite': 4,\n",
       " 'grow': 4,\n",
       " 'spit': 4,\n",
       " 'tie': 4,\n",
       " 'lip': 4,\n",
       " 'weed': 4,\n",
       " 'knee': 4,\n",
       " 'together': 4,\n",
       " 'bloody': 4,\n",
       " 'hittin': 4,\n",
       " 'mall': 4,\n",
       " 'salty': 4,\n",
       " 'boat': 4,\n",
       " 'royce': 4,\n",
       " 'groove': 4,\n",
       " 'cause': 4,\n",
       " 'blunt': 4,\n",
       " 'swervin': 4,\n",
       " 'milli': 4,\n",
       " 'legit-ly': 4,\n",
       " 'stallion': 4,\n",
       " 'pole': 4,\n",
       " 'shoulder': 4,\n",
       " 'leather': 4,\n",
       " 'jacket': 4,\n",
       " 'michael': 4,\n",
       " 'everyday': 4,\n",
       " 'win': 4,\n",
       " 'track': 4,\n",
       " 'anywhere': 4,\n",
       " 'usually': 4,\n",
       " 'angel': 4,\n",
       " 'fresh': 4,\n",
       " 'crowd': 4,\n",
       " 'thinkin': 4,\n",
       " 'prada': 4,\n",
       " 'trips': 4,\n",
       " 'fashion': 4,\n",
       " 'floor-floor': 4,\n",
       " 'ghost': 4,\n",
       " 'regret': 4,\n",
       " 'pillow': 4,\n",
       " 'noisy': 4,\n",
       " 'regular': 4,\n",
       " 'heaven': 4,\n",
       " 'idea': 4,\n",
       " 'eh': 4,\n",
       " 'discreet': 4,\n",
       " 'thot': 4,\n",
       " 'motherfuckin': 4,\n",
       " 'lambo': 4,\n",
       " 'rockin': 4,\n",
       " 'upon': 4,\n",
       " 'tree': 4,\n",
       " 'tight': 4,\n",
       " 'joc': 4,\n",
       " 'jewelry': 4,\n",
       " 'comma': 4,\n",
       " 'period': 4,\n",
       " 'bm': 4,\n",
       " 'follow': 4,\n",
       " 'litty': 4,\n",
       " 'jet': 4,\n",
       " 'belong': 4,\n",
       " 'wishin': 4,\n",
       " 'neighbourhood': 4,\n",
       " 'sunshine': 4,\n",
       " 'louis': 4,\n",
       " 'carpet': 4,\n",
       " 'heat': 4,\n",
       " 'tommy': 4,\n",
       " 'book': 4,\n",
       " 'wifey': 4,\n",
       " 'pip': 4,\n",
       " 'nerve': 4,\n",
       " 'naw': 4,\n",
       " 'trade': 4,\n",
       " 'settle': 4,\n",
       " 'that-that': 4,\n",
       " 'proof': 4,\n",
       " 'apart': 4,\n",
       " 'ecstasy': 4,\n",
       " 'overnight': 4,\n",
       " 'slave': 4,\n",
       " 'hurts': 4,\n",
       " 'due': 4,\n",
       " 'extortion': 4,\n",
       " 'literally': 4,\n",
       " 'baddie': 4,\n",
       " 'fatty': 4,\n",
       " 'leavin': 4,\n",
       " 'rrr': 4,\n",
       " 'f-': 4,\n",
       " 'loser': 4,\n",
       " 'quitter': 4,\n",
       " 'lejos': 4,\n",
       " 'mix': 4,\n",
       " 'exhale': 4,\n",
       " 'wishing': 4,\n",
       " 'fed': 4,\n",
       " 'account': 4,\n",
       " 'stage': 4,\n",
       " 'hopefully': 4,\n",
       " 'dropout': 4,\n",
       " 'annoy': 4,\n",
       " 'tryin': 3,\n",
       " 'sin': 3,\n",
       " 'seasons': 3,\n",
       " 'dare': 3,\n",
       " 'swat': 3,\n",
       " 'trappin': 3,\n",
       " 'app': 3,\n",
       " 'private': 3,\n",
       " 'thinking': 3,\n",
       " 'lamborghini': 3,\n",
       " 'hip': 3,\n",
       " 'rockstar': 3,\n",
       " 'chopper': 3,\n",
       " 'g': 3,\n",
       " 'codeine': 3,\n",
       " 'flip': 3,\n",
       " 'wind': 3,\n",
       " 'maybach': 3,\n",
       " 'solo': 3,\n",
       " 'murder': 3,\n",
       " 'write': 3,\n",
       " 'paradise': 3,\n",
       " 'strawberry': 3,\n",
       " 'lipstick': 3,\n",
       " 'state': 3,\n",
       " 'brown': 3,\n",
       " 'lately': 3,\n",
       " 'workin': 3,\n",
       " 'virgil': 3,\n",
       " 'size': 3,\n",
       " 'spain': 3,\n",
       " 'domain': 3,\n",
       " 'audemar': 3,\n",
       " 'dropped': 3,\n",
       " ...}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re \n",
    "from collections import OrderedDict\n",
    "import json\n",
    "\n",
    "f = open('song_us_cut.json',)\n",
    " \n",
    "data = json.load(f)\n",
    "\n",
    "data1 = data\n",
    "\n",
    "\n",
    "import csv\n",
    "\n",
    "stop_words = ['I', '妳', ]\n",
    "with open('tw_stopwords.csv', newline='') as inputfile:\n",
    "    for row in csv.reader(inputfile):\n",
    "        stop_words.append(row[0])\n",
    "\n",
    "with open('en_stopwords.csv', newline='') as inputfile:\n",
    "    for row in csv.reader(inputfile):\n",
    "        stop_words.append(row[0])\n",
    "\n",
    "for i in data1.copy().keys():\n",
    "    if i in stop_words or re.match('\\W|.*\\'\\.*|\\d', i):\n",
    "        del data1[i]\n",
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de82a810-1c37-46ab-9b3f-0be5a2a6c3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonString = json.dumps(x)\n",
    "jsonFile = open(\"song_tw_test.json\", \"w\")\n",
    "jsonFile.write(jsonString)\n",
    "jsonFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2090f714-ec32-4e08-8caf-cecfb5bc6a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "import json\n",
    "\n",
    "f = open('song_us_cut_cleaned.json',)\n",
    " \n",
    "data = json.load(f)\n",
    "\n",
    "x = list(data.items())\n",
    "x = dict(x)\n",
    "x = dict({\"characters\":list(x.keys()), \"count\":list(x.values())})\n",
    "# jsonString = json.load(x)\n",
    "x = pd.DataFrame(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "992b0730-9033-479f-8565-cb00b58b4860",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.to_csv(\"song_us_cut_cleaned.csv\", index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ea507731-74ce-4e79-b2e2-793698e8bf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "x2 = x.to_csv(index=0)\n",
    "\n",
    "filename = \"test.csv\"\n",
    "    \n",
    "# writing to csv file \n",
    "with open(filename, 'w') as csvfile: \n",
    "    # creating a csv writer object \n",
    "    csvwriter = csv.writer(csvfile) \n",
    "        \n",
    "#     # writing the fields \n",
    "    csvwriter.writerow(x2) \n",
    "        \n",
    "#     # writing the data rows \n",
    "#     csvwriter.writerows(rows)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
